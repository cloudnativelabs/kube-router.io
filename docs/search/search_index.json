{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Kube-router Documentation The documentation is divided into the following sections: Introduction Introduction What is Kube-router? Why Kube-router? Concepts See it in action How it works? Architecture User Guide User Guide installation requirements Operations guide Health Observability Troubleshooting Pod toolbox Upgrades IPv6 / Dual-Stack Load Balancer Support Developer and Contributor Guide Developer Guide Contributor Guideline","title":"Overview"},{"location":"#welcome-to-kube-router-documentation","text":"The documentation is divided into the following sections:","title":"Welcome to Kube-router Documentation"},{"location":"#introduction","text":"Introduction What is Kube-router? Why Kube-router?","title":"Introduction"},{"location":"#concepts","text":"See it in action How it works? Architecture","title":"Concepts"},{"location":"#user-guide","text":"User Guide installation requirements","title":"User Guide"},{"location":"#operations-guide","text":"Health Observability Troubleshooting Pod toolbox Upgrades IPv6 / Dual-Stack Load Balancer Support","title":"Operations guide"},{"location":"#developer-and-contributor-guide","text":"Developer Guide Contributor Guideline","title":"Developer and Contributor Guide"},{"location":"CONTRIBUTING/","text":"Contributing to Kube-router Summary This document covers how to contribute to the kube-router project. Kube-router uses github PRs to manage contributions (could be anything from documentation, bug fixes, manifests etc.). Please read users guide and developers guide for the functionality and internals of kube-router. Filing issues If you have a question about Kube-router or have a problem using it, please start with contacting us on community forum for quick help. If that doesn't answer your questions, or if you think you found a bug, please file an issue . Contributing Changes Fork the code Navigate to: https://github.com/cloudnativelabs/kube-router and fork the repository. Follow these steps to setup a local repository for working on Kube-router: $ git clone https://github.com/YOUR_ACCOUNT/kube-router.git $ cd kube-router $ git remote add upstream https://github.com/cloudnativelabs/kube-router $ git checkout master $ git fetch upstream $ git rebase upstream/master Creating A Feature Branch Create a new branch to make changes on and that branch. $ git checkout -b feature_x (make your changes) $ git status $ git add . $ git commit -a -m \"descriptive commit message for your changes\" get update from upstream $ git checkout master $ git fetch upstream $ git rebase upstream/master $ git checkout feature_x $ git rebase master Now your feature_x branch is up-to-date with all the code in upstream/master , so push to your fork Performing A Pull Request $ git push origin master $ git push origin feature_x Now that the feature_x branch has been pushed to your GitHub repository, you can initiate the pull request.","title":"Contributing to Kube-router"},{"location":"CONTRIBUTING/#contributing-to-kube-router","text":"","title":"Contributing to Kube-router"},{"location":"CONTRIBUTING/#summary","text":"This document covers how to contribute to the kube-router project. Kube-router uses github PRs to manage contributions (could be anything from documentation, bug fixes, manifests etc.). Please read users guide and developers guide for the functionality and internals of kube-router.","title":"Summary"},{"location":"CONTRIBUTING/#filing-issues","text":"If you have a question about Kube-router or have a problem using it, please start with contacting us on community forum for quick help. If that doesn't answer your questions, or if you think you found a bug, please file an issue .","title":"Filing issues"},{"location":"CONTRIBUTING/#contributing-changes","text":"","title":"Contributing Changes"},{"location":"CONTRIBUTING/#fork-the-code","text":"Navigate to: https://github.com/cloudnativelabs/kube-router and fork the repository. Follow these steps to setup a local repository for working on Kube-router: $ git clone https://github.com/YOUR_ACCOUNT/kube-router.git $ cd kube-router $ git remote add upstream https://github.com/cloudnativelabs/kube-router $ git checkout master $ git fetch upstream $ git rebase upstream/master","title":"Fork the code"},{"location":"CONTRIBUTING/#creating-a-feature-branch","text":"Create a new branch to make changes on and that branch. $ git checkout -b feature_x (make your changes) $ git status $ git add . $ git commit -a -m \"descriptive commit message for your changes\" get update from upstream $ git checkout master $ git fetch upstream $ git rebase upstream/master $ git checkout feature_x $ git rebase master Now your feature_x branch is up-to-date with all the code in upstream/master , so push to your fork","title":"Creating A Feature Branch"},{"location":"CONTRIBUTING/#performing-a-pull-request","text":"$ git push origin master $ git push origin feature_x Now that the feature_x branch has been pushed to your GitHub repository, you can initiate the pull request.","title":"Performing A Pull Request"},{"location":"RELEASE/","text":"Process for creating a kube-router release Preparing for the release Ensure that the Golang release used is still supported. Definition happens currently in Github Workflow. Ensure that the Alpine version used in container builds is still supported. Definition happens currently in Github Workflow. Ensure that Golang dependencies are updated. go list -mod=mod -u -m -f '{{.}}{{if .Indirect}} IAMINDIRECT{{end}}' all | grep -v IAMINDIRECT lists possible updates. Ensure that the GoBGP version is updated. See upstream and GoBGP definition in Makefile and go.mod. Ensure that the Kubernetes object definitions do not contain deprecated object types. Definition currently is in kube-router's daemonset folder. New major/minor release Create a branch named v$MAJOR.$MINOR from the default branch (currently: master) Create a new tag with the release tag v$MAJOR.$MINOR.0 git tag <tag_name> git push origin <tag_name> Note: your remote for the main kube-router repo may not be origin, please correct it to whatever you have called the official kube-router remote. New patch release Change to the master branch Use git log to identify which commits you want to bring to the new patch release Change to the major/minor release branch that was created for this release Cherry-Pick the changes from the master branch into the release branch Create a new tag from the v$MAJOR.$MINOR release branch with the release tag v$MAJOR.$MINOR.$PATCH Example: git checkout master git log --color --pretty=format:'%h - %s (%cr) <%an>' --abbrev-commit --decorate git checkout <release_branch> git cherry-pick <commit_hash_from_above_log> git tag <tag_name> git push origin <tag_name> Note: your remote for the main kube-router repo may not be origin, please correct it to whatever you have called the official kube-router remote. Release Candidates Follow above instructions and ensure that the tag contains -rc . Don't mark the pre-release as a proper release. Release Build Process Once the tag is pushed to GitHub GitHub Actions will be triggered and several things will happen: kube-router will be linted kube-router will be tested The actions will run a test build of the kube-router binary Containers for defined architectures (see platforms section in yaml) will be built and pushed to DockerHub via the docker buildx command goreleaser will be run and will: Generate a draft release on GitHub where maintainers can later choose to update it and release it Brief release notes will be added to the draft release Build all of the binary releases for defined architectures and attach them to the draft release on GitHub After the release Go to the GitHub releases page for the kube-router project Find the draft release Consistent Changelog Syntax can be retrieved by running the following Git command: git log --format='* %h - %s `<%an>`' <tag>..<tag> Announce the release in #kube-router on Kubernetes Slack.","title":"Process for creating a kube-router release"},{"location":"RELEASE/#process-for-creating-a-kube-router-release","text":"","title":"Process for creating a kube-router release"},{"location":"RELEASE/#preparing-for-the-release","text":"Ensure that the Golang release used is still supported. Definition happens currently in Github Workflow. Ensure that the Alpine version used in container builds is still supported. Definition happens currently in Github Workflow. Ensure that Golang dependencies are updated. go list -mod=mod -u -m -f '{{.}}{{if .Indirect}} IAMINDIRECT{{end}}' all | grep -v IAMINDIRECT lists possible updates. Ensure that the GoBGP version is updated. See upstream and GoBGP definition in Makefile and go.mod. Ensure that the Kubernetes object definitions do not contain deprecated object types. Definition currently is in kube-router's daemonset folder.","title":"Preparing for the release"},{"location":"RELEASE/#new-majorminor-release","text":"Create a branch named v$MAJOR.$MINOR from the default branch (currently: master) Create a new tag with the release tag v$MAJOR.$MINOR.0 git tag <tag_name> git push origin <tag_name> Note: your remote for the main kube-router repo may not be origin, please correct it to whatever you have called the official kube-router remote.","title":"New major/minor release"},{"location":"RELEASE/#new-patch-release","text":"Change to the master branch Use git log to identify which commits you want to bring to the new patch release Change to the major/minor release branch that was created for this release Cherry-Pick the changes from the master branch into the release branch Create a new tag from the v$MAJOR.$MINOR release branch with the release tag v$MAJOR.$MINOR.$PATCH Example: git checkout master git log --color --pretty=format:'%h - %s (%cr) <%an>' --abbrev-commit --decorate git checkout <release_branch> git cherry-pick <commit_hash_from_above_log> git tag <tag_name> git push origin <tag_name> Note: your remote for the main kube-router repo may not be origin, please correct it to whatever you have called the official kube-router remote.","title":"New patch release"},{"location":"RELEASE/#release-candidates","text":"Follow above instructions and ensure that the tag contains -rc . Don't mark the pre-release as a proper release.","title":"Release Candidates"},{"location":"RELEASE/#release-build-process","text":"Once the tag is pushed to GitHub GitHub Actions will be triggered and several things will happen: kube-router will be linted kube-router will be tested The actions will run a test build of the kube-router binary Containers for defined architectures (see platforms section in yaml) will be built and pushed to DockerHub via the docker buildx command goreleaser will be run and will: Generate a draft release on GitHub where maintainers can later choose to update it and release it Brief release notes will be added to the draft release Build all of the binary releases for defined architectures and attach them to the draft release on GitHub","title":"Release Build Process"},{"location":"RELEASE/#after-the-release","text":"Go to the GitHub releases page for the kube-router project Find the draft release Consistent Changelog Syntax can be retrieved by running the following Git command: git log --format='* %h - %s `<%an>`' <tag>..<tag> Announce the release in #kube-router on Kubernetes Slack.","title":"After the release"},{"location":"architecture/","text":"Architecture Kube-router is built around concept of watchers and controllers. Watchers use Kubernetes watch API to get notification on events related to create, update, delete of Kubernetes objects. Each watcher gets notification related to a particular API object. On receiving an event from API server, watcher broadcasts events. Controller registers to get event updates from the watchers and act up on the events. Kube-router consists of 3 core controllers and multiple watchers as depicted in below diagram. Each of the controller follows below structure: func Run() { for { Sync() // control loop that runs for ever and perfom sync at periodic interval } } func OnUpdate() { Sync() // on receiving update of a watched API object (namespace, node, pod, network policy etc) } Sync() { //re-concile any state changes } Cleanup() { // cleanup any changes (to iptables, ipvs, network etc) done to the system }","title":"Architecture"},{"location":"architecture/#architecture","text":"Kube-router is built around concept of watchers and controllers. Watchers use Kubernetes watch API to get notification on events related to create, update, delete of Kubernetes objects. Each watcher gets notification related to a particular API object. On receiving an event from API server, watcher broadcasts events. Controller registers to get event updates from the watchers and act up on the events. Kube-router consists of 3 core controllers and multiple watchers as depicted in below diagram. Each of the controller follows below structure: func Run() { for { Sync() // control loop that runs for ever and perfom sync at periodic interval } } func OnUpdate() { Sync() // on receiving update of a watched API object (namespace, node, pod, network policy etc) } Sync() { //re-concile any state changes } Cleanup() { // cleanup any changes (to iptables, ipvs, network etc) done to the system }","title":"Architecture"},{"location":"bgp/","text":"Configuring BGP Peers When kube-router is used to provide pod-to-pod networking, BGP is used to exchange routes across the nodes. Kube-router provides flexible networking models to support different deployments (public vs private cloud, routable vs non-routable pod IPs, service IPs, etc.). Peering Within The Cluster Full Node-To-Node Mesh This is the default mode. All nodes in the clusters form iBGP peering relationships with rest of the nodes forming a full node-to-node mesh. Each node advertise the pod CIDR allocated to the nodes with its peers (the rest of the nodes in the cluster). There is no configuration required in this mode. All the nodes in the cluster are associated with the private ASN 64512 implicitly (which can be configured with --cluster-asn flag) and users are transparent to use of iBGP. This mode is suitable in public cloud environments or small cluster deployments. Node-To-Node Peering Without Full Mesh This model is used to support more than a single AS per cluster to allow for an AS per rack or an AS per node. Nodes in the cluster do not form full node-to-node meshes. Users have to explicitly select this mode by specifying --nodes-full-mesh=false when launching kube-router. In this mode kube-router expects each node will be configured with an ASN number from the node's API object annotations. Kube-router will use the node's kube-router.io/node.asn annotation value as the ASN number for the node. Users can annotate node objects with the following command: kubectl annotate node <kube-node> \"kube-router.io/node.asn=64512\" Only nodes within same ASN form full mesh. Two nodes with different ASNs never get peered. Route-Reflector setup Without Full Mesh This model supports the common scheme of using a Route Reflector Server node to concentrate peering from client peers. This has the big advantage of not needing full mesh, and will scale better. In this mode kube-router expects each node is configured either in Route Reflector server mode or in Route Reflector client mode. This is done with node kube-router.io/rr.server=ClusterID , kube-router.io/rr.client=ClusterId respectively. In this mode each route reflector client will only peer with route reflector servers. Each route reflector server will only peer with other route reflector servers and with route reflector clients enabling reflection. Users can annotate node objects with the following command for Route Reflector server mode: kubectl annotate node <kube-node> \"kube-router.io/rr.server=42\" and for Route Reflector client mode: kubectl annotate node <kube-node> \"kube-router.io/rr.client=42\" Only nodes with the same ClusterID in client and server mode will peer together. When joining new nodes to the cluster, remember to annotate them with kube-router.io/rr.client=42 , and then restart kube-router on the new nodes and the route reflector server nodes to let them successfully read the annotations and peer with each other. Peering Outside The Cluster Global External BGP Peers An optional global BGP peer can be configured by specifying the parameters: --peer-router-asns and --peer-router-ips . When configured each node in the cluster forms a peer relationship with specified global peer. Pod CIDR and Cluster IPs get advertised to the global BGP peer. For redundancy, you can also configure more than one peer router by specifying a slice of BGP peers. For example: --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=65000,65000 Node Specific External BGP Peers Alternatively, each node can be configured with one or more node specific BGP peers. Information regarding node specific BGP peer is read from node API object annotations: kube-router.io/peer.ips kube-router.io/peer.asns For example, users can annotate node object with below commands: kubectl annotate node <kube-node> \"kube-router.io/peer.ips=192.168.1.99,192.168.1.100\" kubectl annotate node <kube-node> \"kube-router.io/peer.asns=65000,65000\" AS Path Prepending For traffic shaping purposes, you may want to prepend the AS path announced to peers. This can be accomplished on a per-node basis with annotations: kube-router.io/path-prepend.as kube-router.io/path-prepend.repeat-n If you wanted to prepend all routes from a particular node with the AS 65000 five times, you would run the following commands: kubectl annotate node <kube-node> \"kube-router.io/path-prepend.as=65000\" kubectl annotate node <kube-node> \"kube-router.io/path-prepend.repeat-n=5\" BGP Peer Local IP configuration In some setups it might be desirable to set a local IP address used for connecting external BGP peers. This can be accomplished on nodes with annotations: kube-router.io/peer.localips If set, this must be a list with a local IP address for each peer, or left empty to use nodeIP. Example: kubectl annotate node <kube-node> \"kube-router.io/peer.localips=10.1.1.1,10.1.1.2\" This will instruct kube-router to use IP 10.1.1.1 for first BGP peer as a local address, and use 10.1.1.2 for the second. BGP Peer Password Authentication The examples above have assumed there is no password authentication with BGP peer routers. If you need to use a password for peering, you can use the --peer-router-passwords command-line option, the kube-router.io/peer.passwords node annotation, or the --peer-router-passwords-file command-line option. Base64 Encoding Passwords To ensure passwords are easily parsed, but not easily read by human eyes, kube-router requires that they are encoded as base64. On a Linux or MacOS system you can encode your passwords on the command line: $ printf \"SecurePassword\" | base64 U2VjdXJlUGFzc3dvcmQ= Password Configuration Examples In this CLI flag example the first router (192.168.1.99) uses a password, while the second (192.168.1.100) does not. --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=\"65000,65000\" --peer-router-passwords=\"U2VjdXJlUGFzc3dvcmQK,\" Note the comma indicating the end of the first password. Here's the same example but configured as node annotations: kubectl annotate node <kube-node> \"kube-router.io/peer.ips=192.168.1.99,192.168.1.100\" kubectl annotate node <kube-node> \"kube-router.io/peer.asns=65000,65000\" kubectl annotate node <kube-node> \"kube-router.io/peer.passwords=U2VjdXJlUGFzc3dvcmQK,\" Finally, to include peer passwords as a file you would run kube-router with the following option: --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=\"65000,65000\" --peer-router-passwords-file=\"/etc/kube-router/bgp-passwords.conf\" The password file, closely follows the syntax of the command-line and node annotation options. Here, the first peer IP (192.168.1.99) would be configured with a password, while the second would not. U2VjdXJlUGFzc3dvcmQK, Note, complex parsing is not done on this file, please do not include any content other than the passwords on a single line in this file. BGP Communities Global peers support the addition of BGP communities via node annotations. Node annotations can be formulated either as: a single 32-bit integer two 16-bit integers separated by a colon ( : ) common BGP community names (e.g. no-export , internet , no-peer , etc.) (see: WellKnownCommunityNameMap ) In the following example we add the NO_EXPORT BGP community to two of our nodes via annotation using all three forms of the annotation: kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=4294967041\" kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=65535:65281\" kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=no-export\" Custom BGP Import Policy Reject kube-router, by default, accepts all routes advertised by its neighbors. If the bgp session with one neighbor dies, GoBGP deletes all routes received by it. If one of the received routes is needed for this node to function properly (eg: custom static route), it could stop working. In the following example we add custom prefixes that'll be set via a custom import policy reject rule annotation, protecting the node from losing required routes: kubectl annotate node <kube-node> \"kube-router.io/node.bgp.customimportreject=10.0.0.0/16, 192.168.1.0/24\" BGP listen address list By default, the GoBGP server binds on the node IP address. However, in some cases nodes with multiple IP addresses desire to bind GoBGP to multiple local addresses. Local IP addresses on which GoGBP should listen on a node can be configured with annotation kube-router.io/bgp-local-addresses . Here is sample example to make GoBGP server to listen on multiple IP address: kubectl annotate node ip-172-20-46-87.us-west-2.compute.internal \"kube-router.io/bgp-local-addresses=172.20.56.25,192.168.1.99\" Overriding the next hop By default, kube-router populates the GoBGP RIB with node IP as next hop for the advertised pod CIDRs and service VIPs. While this works for most cases, overriding the next hop for the advertised routes is necessary when node has multiple interfaces over which external peers are reached. Next hops need to be defined as the interface over which external peer can be reached. Setting --override-nexthop to true leverages the BGP next-hop-self functionality implemented in GoBGP. The next hop will automatically be selected appropriately when advertising routes, irrespective of the next hop in the RIB. Overriding the next hop and enable IPIP/tunnel A common scenario exists where each node in the cluster is connected to two upstream routers that are in two different subnets. For example, one router is connected to a public network subnet and the other router is connected to a private network subnet. Additionally, nodes may be split across different subnets (e.g. different racks) each of which has their own routers. In this scenario, --override-nexthop can be used to correctly peer with each upstream router, ensuring that the BGP next-hop attribute is correctly set to the node's IP address that faces the upstream router. The --enable-overlay option can be set to allow overlay/underlay tunneling across the different subnets to achieve an interconnected pod network. This configuration would have the following effects: Peering Outside the Cluster via one of themany means that kube-router makes available Overriding Next Hop Enabling overlays in either full mode or with nodes in different subnets The warning here is that when using --override-nexthop in the above scenario, it may cause kube-router to advertise an IP address other than the node IP which is what kube-router connects the tunnel to when the --enable-overlay option is given. If this happens it may cause some network flows to become un-routable. Specifically, people need to take care when combining --override-nexthop and --enable-overlay and make sure that they understand their network, the flows they desire, how the kube-router logic works, and the possible side effects that are created from their configuration. Please refer to this PR for the risk and impact discussion.","title":"BGP"},{"location":"bgp/#configuring-bgp-peers","text":"When kube-router is used to provide pod-to-pod networking, BGP is used to exchange routes across the nodes. Kube-router provides flexible networking models to support different deployments (public vs private cloud, routable vs non-routable pod IPs, service IPs, etc.).","title":"Configuring BGP Peers"},{"location":"bgp/#peering-within-the-cluster","text":"","title":"Peering Within The Cluster"},{"location":"bgp/#full-node-to-node-mesh","text":"This is the default mode. All nodes in the clusters form iBGP peering relationships with rest of the nodes forming a full node-to-node mesh. Each node advertise the pod CIDR allocated to the nodes with its peers (the rest of the nodes in the cluster). There is no configuration required in this mode. All the nodes in the cluster are associated with the private ASN 64512 implicitly (which can be configured with --cluster-asn flag) and users are transparent to use of iBGP. This mode is suitable in public cloud environments or small cluster deployments.","title":"Full Node-To-Node Mesh"},{"location":"bgp/#node-to-node-peering-without-full-mesh","text":"This model is used to support more than a single AS per cluster to allow for an AS per rack or an AS per node. Nodes in the cluster do not form full node-to-node meshes. Users have to explicitly select this mode by specifying --nodes-full-mesh=false when launching kube-router. In this mode kube-router expects each node will be configured with an ASN number from the node's API object annotations. Kube-router will use the node's kube-router.io/node.asn annotation value as the ASN number for the node. Users can annotate node objects with the following command: kubectl annotate node <kube-node> \"kube-router.io/node.asn=64512\" Only nodes within same ASN form full mesh. Two nodes with different ASNs never get peered.","title":"Node-To-Node Peering Without Full Mesh"},{"location":"bgp/#route-reflector-setup-without-full-mesh","text":"This model supports the common scheme of using a Route Reflector Server node to concentrate peering from client peers. This has the big advantage of not needing full mesh, and will scale better. In this mode kube-router expects each node is configured either in Route Reflector server mode or in Route Reflector client mode. This is done with node kube-router.io/rr.server=ClusterID , kube-router.io/rr.client=ClusterId respectively. In this mode each route reflector client will only peer with route reflector servers. Each route reflector server will only peer with other route reflector servers and with route reflector clients enabling reflection. Users can annotate node objects with the following command for Route Reflector server mode: kubectl annotate node <kube-node> \"kube-router.io/rr.server=42\" and for Route Reflector client mode: kubectl annotate node <kube-node> \"kube-router.io/rr.client=42\" Only nodes with the same ClusterID in client and server mode will peer together. When joining new nodes to the cluster, remember to annotate them with kube-router.io/rr.client=42 , and then restart kube-router on the new nodes and the route reflector server nodes to let them successfully read the annotations and peer with each other.","title":"Route-Reflector setup  Without Full Mesh"},{"location":"bgp/#peering-outside-the-cluster","text":"","title":"Peering Outside The Cluster"},{"location":"bgp/#global-external-bgp-peers","text":"An optional global BGP peer can be configured by specifying the parameters: --peer-router-asns and --peer-router-ips . When configured each node in the cluster forms a peer relationship with specified global peer. Pod CIDR and Cluster IPs get advertised to the global BGP peer. For redundancy, you can also configure more than one peer router by specifying a slice of BGP peers. For example: --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=65000,65000","title":"Global External BGP Peers"},{"location":"bgp/#node-specific-external-bgp-peers","text":"Alternatively, each node can be configured with one or more node specific BGP peers. Information regarding node specific BGP peer is read from node API object annotations: kube-router.io/peer.ips kube-router.io/peer.asns For example, users can annotate node object with below commands: kubectl annotate node <kube-node> \"kube-router.io/peer.ips=192.168.1.99,192.168.1.100\" kubectl annotate node <kube-node> \"kube-router.io/peer.asns=65000,65000\"","title":"Node Specific External BGP Peers"},{"location":"bgp/#as-path-prepending","text":"For traffic shaping purposes, you may want to prepend the AS path announced to peers. This can be accomplished on a per-node basis with annotations: kube-router.io/path-prepend.as kube-router.io/path-prepend.repeat-n If you wanted to prepend all routes from a particular node with the AS 65000 five times, you would run the following commands: kubectl annotate node <kube-node> \"kube-router.io/path-prepend.as=65000\" kubectl annotate node <kube-node> \"kube-router.io/path-prepend.repeat-n=5\"","title":"AS Path Prepending"},{"location":"bgp/#bgp-peer-local-ip-configuration","text":"In some setups it might be desirable to set a local IP address used for connecting external BGP peers. This can be accomplished on nodes with annotations: kube-router.io/peer.localips If set, this must be a list with a local IP address for each peer, or left empty to use nodeIP. Example: kubectl annotate node <kube-node> \"kube-router.io/peer.localips=10.1.1.1,10.1.1.2\" This will instruct kube-router to use IP 10.1.1.1 for first BGP peer as a local address, and use 10.1.1.2 for the second.","title":"BGP Peer Local IP configuration"},{"location":"bgp/#bgp-peer-password-authentication","text":"The examples above have assumed there is no password authentication with BGP peer routers. If you need to use a password for peering, you can use the --peer-router-passwords command-line option, the kube-router.io/peer.passwords node annotation, or the --peer-router-passwords-file command-line option.","title":"BGP Peer Password Authentication"},{"location":"bgp/#base64-encoding-passwords","text":"To ensure passwords are easily parsed, but not easily read by human eyes, kube-router requires that they are encoded as base64. On a Linux or MacOS system you can encode your passwords on the command line: $ printf \"SecurePassword\" | base64 U2VjdXJlUGFzc3dvcmQ=","title":"Base64 Encoding Passwords"},{"location":"bgp/#password-configuration-examples","text":"In this CLI flag example the first router (192.168.1.99) uses a password, while the second (192.168.1.100) does not. --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=\"65000,65000\" --peer-router-passwords=\"U2VjdXJlUGFzc3dvcmQK,\" Note the comma indicating the end of the first password. Here's the same example but configured as node annotations: kubectl annotate node <kube-node> \"kube-router.io/peer.ips=192.168.1.99,192.168.1.100\" kubectl annotate node <kube-node> \"kube-router.io/peer.asns=65000,65000\" kubectl annotate node <kube-node> \"kube-router.io/peer.passwords=U2VjdXJlUGFzc3dvcmQK,\" Finally, to include peer passwords as a file you would run kube-router with the following option: --peer-router-ips=\"192.168.1.99,192.168.1.100\" --peer-router-asns=\"65000,65000\" --peer-router-passwords-file=\"/etc/kube-router/bgp-passwords.conf\" The password file, closely follows the syntax of the command-line and node annotation options. Here, the first peer IP (192.168.1.99) would be configured with a password, while the second would not. U2VjdXJlUGFzc3dvcmQK, Note, complex parsing is not done on this file, please do not include any content other than the passwords on a single line in this file.","title":"Password Configuration Examples"},{"location":"bgp/#bgp-communities","text":"Global peers support the addition of BGP communities via node annotations. Node annotations can be formulated either as: a single 32-bit integer two 16-bit integers separated by a colon ( : ) common BGP community names (e.g. no-export , internet , no-peer , etc.) (see: WellKnownCommunityNameMap ) In the following example we add the NO_EXPORT BGP community to two of our nodes via annotation using all three forms of the annotation: kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=4294967041\" kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=65535:65281\" kubectl annotate node <kube-node> \"kube-router.io/node.bgp.communities=no-export\"","title":"BGP Communities"},{"location":"bgp/#custom-bgp-import-policy-reject","text":"kube-router, by default, accepts all routes advertised by its neighbors. If the bgp session with one neighbor dies, GoBGP deletes all routes received by it. If one of the received routes is needed for this node to function properly (eg: custom static route), it could stop working. In the following example we add custom prefixes that'll be set via a custom import policy reject rule annotation, protecting the node from losing required routes: kubectl annotate node <kube-node> \"kube-router.io/node.bgp.customimportreject=10.0.0.0/16, 192.168.1.0/24\"","title":"Custom BGP Import Policy Reject"},{"location":"bgp/#bgp-listen-address-list","text":"By default, the GoBGP server binds on the node IP address. However, in some cases nodes with multiple IP addresses desire to bind GoBGP to multiple local addresses. Local IP addresses on which GoGBP should listen on a node can be configured with annotation kube-router.io/bgp-local-addresses . Here is sample example to make GoBGP server to listen on multiple IP address: kubectl annotate node ip-172-20-46-87.us-west-2.compute.internal \"kube-router.io/bgp-local-addresses=172.20.56.25,192.168.1.99\"","title":"BGP listen address list"},{"location":"bgp/#overriding-the-next-hop","text":"By default, kube-router populates the GoBGP RIB with node IP as next hop for the advertised pod CIDRs and service VIPs. While this works for most cases, overriding the next hop for the advertised routes is necessary when node has multiple interfaces over which external peers are reached. Next hops need to be defined as the interface over which external peer can be reached. Setting --override-nexthop to true leverages the BGP next-hop-self functionality implemented in GoBGP. The next hop will automatically be selected appropriately when advertising routes, irrespective of the next hop in the RIB.","title":"Overriding the next hop"},{"location":"bgp/#overriding-the-next-hop-and-enable-ipiptunnel","text":"A common scenario exists where each node in the cluster is connected to two upstream routers that are in two different subnets. For example, one router is connected to a public network subnet and the other router is connected to a private network subnet. Additionally, nodes may be split across different subnets (e.g. different racks) each of which has their own routers. In this scenario, --override-nexthop can be used to correctly peer with each upstream router, ensuring that the BGP next-hop attribute is correctly set to the node's IP address that faces the upstream router. The --enable-overlay option can be set to allow overlay/underlay tunneling across the different subnets to achieve an interconnected pod network. This configuration would have the following effects: Peering Outside the Cluster via one of themany means that kube-router makes available Overriding Next Hop Enabling overlays in either full mode or with nodes in different subnets The warning here is that when using --override-nexthop in the above scenario, it may cause kube-router to advertise an IP address other than the node IP which is what kube-router connects the tunnel to when the --enable-overlay option is given. If this happens it may cause some network flows to become un-routable. Specifically, people need to take care when combining --override-nexthop and --enable-overlay and make sure that they understand their network, the flows they desire, how the kube-router logic works, and the possible side effects that are created from their configuration. Please refer to this PR for the risk and impact discussion.","title":"Overriding the next hop and enable IPIP/tunnel"},{"location":"developing/","text":"Developer's Guide We aim to make local development and testing as straightforward as possible. For basic guidelines around contributing, see the CONTRIBUTING document. There are a number of automation tools available to help with testing and building your changes, detailed below. Building kube-router Go version 1.19 or above is required to build kube-router All the dependencies are specified as Go modules and will be fetched into your cache, so just run make kube-router or go build pkg/cmd/kube-router.go to build. Building A Docker Image Running make container will compile kube-router (if needed) and build a Docker image. By default the container will be tagged with the last release version, and current commit ID. For example: $ make container Building for GOARCH=amd64 Verifying kube-router gobgp for ARCH=x86-64 ... Starting kube-router container image build for amd64 on amd64 docker build -t \"cloudnativelabs/kube-router-git:amd64-bug_fixes_for_v2.0.0\" -f Dockerfile --build-arg ARCH=\"\" \\ --build-arg BUILDTIME_BASE=\"golang:1.20.9-alpine3.18\" --build-arg RUNTIME_BASE=\"alpine:3.18\" . Sending build context to Docker daemon 198.6MB Step 1/19 : ARG BUILDTIME_BASE=golang:1-alpine Step 2/19 : ARG RUNTIME_BASE=alpine:latest Step 3/19 : FROM ${BUILDTIME_BASE} as builder ---> 6cbc3ac54aa3 Step 4/19 : ENV BUILD_IN_DOCKER=false ---> Using cache ---> aec11cc4a0cd ... Removing intermediate container 371a162930f5 ---> 1d3f742d559e Step 19/19 : ENTRYPOINT [\"/usr/local/bin/kube-router\"] ---> Running in d5ea6fda9fe4 Removing intermediate container d5ea6fda9fe4 ---> 17cfbc77e293 [Warning] One or more build-args [ARCH] were not consumed Successfully built 17cfbc77e293 Successfully tagged cloudnativelabs/kube-router-git:amd64-bug_fixes_for_v2.0.0 Finished kube-router container image build. The following describes the rest of the portions of the container naming convention kube-router-git indicates that the container was built from git and not from a tag. amd64 indicates that it was built for the amd64 architecture bug_fixes_for_v2.0.0 indicates the branch that the user was on when it was built Pushing A Docker Image Running make push will push your container image to a Docker registry. The default configuration will use the Docker Hub repository for the official kube-router images, cloudnativelabs/kube-router. You can push to a different repository by changing a couple settings, as described in Image Options below. Makefile Options There are several variables which can be modified in the Makefile to customize your builds. They are specified after your make command like this: make OPTION=VALUE . These options can also be set in your environment variables. For more details beyond the scope of this document, see the Makefile and run make help . Image Options You can configure the name and tag of the Docker image with a few variables passed to make container and make push . Example: $ make container IMG_FQDN=quay.io IMG_NAMESPACE=bzub IMG_TAG=custom docker build -t \"quay.io/bzub/kube-router-git:custom\" . Sending build context to Docker daemon 151.5MB Step 1/4 : FROM alpine ---> a41a7446062d Step 2/4 : RUN apk add --no-cache iptables ipset ---> Using cache ---> 30e25a7640de Step 3/4 : COPY kube-router / ---> Using cache ---> c06f78fd02e8 Step 4/4 : ENTRYPOINT /kube-router ---> Using cache ---> 5cfcfe54623e Successfully built 5cfcfe54623e Successfully tagged quay.io/bzub/kube-router-git:custom REGISTRY is derived from other options. Set this to something else to quickly override the Docker image registry used to tag and push images. Note: This will override other variables below that make up the image name/tag. IMG_FQDN should be set if you are not using Docker Hub for images. In the examples above IMG_FQDN is set to quay.io . IMG_NAMESPACE is the Docker registry user or organization. It is used in URLs. Example: quay.io/IMG_NAMESPACE/kube-router NAME goes onto the end of the Docker registry URL that will be used. Example: quay.io/cloudnativelabs/NAME IMG_TAG is used to override the tag of the Docker image being built. DEV_SUFFIX is appended to Docker image names that are not for release. By default these images get a name ending with -git to signify that they are for testing purposes. Example (DEV-SUFFIX=master-latest): quay.io/cloudnativelabs/kube-router-git:master-latest Release Workflow See Release Documentation for more information Dependency Management kube-router uses go modules for managing dependencies see upstream documentation for more information","title":"Developing"},{"location":"developing/#developers-guide","text":"We aim to make local development and testing as straightforward as possible. For basic guidelines around contributing, see the CONTRIBUTING document. There are a number of automation tools available to help with testing and building your changes, detailed below.","title":"Developer's Guide"},{"location":"developing/#building-kube-router","text":"","title":"Building kube-router"},{"location":"developing/#go-version-119-or-above-is-required-to-build-kube-router","text":"All the dependencies are specified as Go modules and will be fetched into your cache, so just run make kube-router or go build pkg/cmd/kube-router.go to build.","title":"Go version 1.19 or above is required to build kube-router"},{"location":"developing/#building-a-docker-image","text":"Running make container will compile kube-router (if needed) and build a Docker image. By default the container will be tagged with the last release version, and current commit ID. For example: $ make container Building for GOARCH=amd64 Verifying kube-router gobgp for ARCH=x86-64 ... Starting kube-router container image build for amd64 on amd64 docker build -t \"cloudnativelabs/kube-router-git:amd64-bug_fixes_for_v2.0.0\" -f Dockerfile --build-arg ARCH=\"\" \\ --build-arg BUILDTIME_BASE=\"golang:1.20.9-alpine3.18\" --build-arg RUNTIME_BASE=\"alpine:3.18\" . Sending build context to Docker daemon 198.6MB Step 1/19 : ARG BUILDTIME_BASE=golang:1-alpine Step 2/19 : ARG RUNTIME_BASE=alpine:latest Step 3/19 : FROM ${BUILDTIME_BASE} as builder ---> 6cbc3ac54aa3 Step 4/19 : ENV BUILD_IN_DOCKER=false ---> Using cache ---> aec11cc4a0cd ... Removing intermediate container 371a162930f5 ---> 1d3f742d559e Step 19/19 : ENTRYPOINT [\"/usr/local/bin/kube-router\"] ---> Running in d5ea6fda9fe4 Removing intermediate container d5ea6fda9fe4 ---> 17cfbc77e293 [Warning] One or more build-args [ARCH] were not consumed Successfully built 17cfbc77e293 Successfully tagged cloudnativelabs/kube-router-git:amd64-bug_fixes_for_v2.0.0 Finished kube-router container image build. The following describes the rest of the portions of the container naming convention kube-router-git indicates that the container was built from git and not from a tag. amd64 indicates that it was built for the amd64 architecture bug_fixes_for_v2.0.0 indicates the branch that the user was on when it was built","title":"Building A Docker Image"},{"location":"developing/#pushing-a-docker-image","text":"Running make push will push your container image to a Docker registry. The default configuration will use the Docker Hub repository for the official kube-router images, cloudnativelabs/kube-router. You can push to a different repository by changing a couple settings, as described in Image Options below.","title":"Pushing A Docker Image"},{"location":"developing/#makefile-options","text":"There are several variables which can be modified in the Makefile to customize your builds. They are specified after your make command like this: make OPTION=VALUE . These options can also be set in your environment variables. For more details beyond the scope of this document, see the Makefile and run make help .","title":"Makefile Options"},{"location":"developing/#image-options","text":"You can configure the name and tag of the Docker image with a few variables passed to make container and make push . Example: $ make container IMG_FQDN=quay.io IMG_NAMESPACE=bzub IMG_TAG=custom docker build -t \"quay.io/bzub/kube-router-git:custom\" . Sending build context to Docker daemon 151.5MB Step 1/4 : FROM alpine ---> a41a7446062d Step 2/4 : RUN apk add --no-cache iptables ipset ---> Using cache ---> 30e25a7640de Step 3/4 : COPY kube-router / ---> Using cache ---> c06f78fd02e8 Step 4/4 : ENTRYPOINT /kube-router ---> Using cache ---> 5cfcfe54623e Successfully built 5cfcfe54623e Successfully tagged quay.io/bzub/kube-router-git:custom REGISTRY is derived from other options. Set this to something else to quickly override the Docker image registry used to tag and push images. Note: This will override other variables below that make up the image name/tag. IMG_FQDN should be set if you are not using Docker Hub for images. In the examples above IMG_FQDN is set to quay.io . IMG_NAMESPACE is the Docker registry user or organization. It is used in URLs. Example: quay.io/IMG_NAMESPACE/kube-router NAME goes onto the end of the Docker registry URL that will be used. Example: quay.io/cloudnativelabs/NAME IMG_TAG is used to override the tag of the Docker image being built. DEV_SUFFIX is appended to Docker image names that are not for release. By default these images get a name ending with -git to signify that they are for testing purposes. Example (DEV-SUFFIX=master-latest): quay.io/cloudnativelabs/kube-router-git:master-latest","title":"Image Options"},{"location":"developing/#release-workflow","text":"See Release Documentation for more information","title":"Release Workflow"},{"location":"developing/#dependency-management","text":"kube-router uses go modules for managing dependencies see upstream documentation for more information","title":"Dependency Management"},{"location":"dsr/","text":"Direct Server Return More Information For a more detailed explanation on how to use Direct Server Return (DSR) to build a highly scalable and available ingress for Kubernetes see the following blog post What is DSR? When enabled, DSR allows the service endpoint to respond directly to the client request, bypassing the service proxy. When DSR is enabled kube-router will use LVS's tunneling mode to achieve this (more on how later). Quick Start You can enable DSR functionality on a per service basis. Requirements: ClusterIP type service has an externalIP set on it or is a LoadBalancer type service kube-router has been started with --service-external-ip-range configured at least once. This option can be specified multiple times for multiple ranges. The external IPs or LoadBalancer IPs must be included in these ranges. kube-router must be run in service proxy mode with --run-service-proxy (this option is defaulted to true if left unspecified) If you are advertising the service outside the cluster --advertise-external-ip must be set If kube-router is deployed as a Kubernetes pod: hostIPC: true must be set for the pod hostPID: true must be set for the pod The container runtime socket must be mounted into the kube-router pod via a hostPath volume mount. /etc/iproute2/rt_tables (or similar) must be read/write mounted into the kube-router pod via a hostPath volume mount. NOTE: since v6.5.0 of iproute2 this file has been moved underneath /usr in either /usr/lib/iproute2/rt_tables or /usr/share/iproute2/rt_tables instead of in /etc so this mount may need to be updated depending on which version of Linux you're deploying against. kube-router will check all 3 locations and use them in order of the above. A pod network that allows for IPIP encapsulated traffic. The most notable exception to this is that Azure does not transit IPIP encapsulated packets on their network. In this scenario, the end-user may be able to get around this issue by enabling FoU ( --overlay-encap=fou ) and full overlay networking ( --overlay-type=full ) options in kube-router. This hasn't been well tested, but it should allow the DSR encapsulated traffic to route correctly. To enable DSR you need to annotate service with the kube-router.io/service.dsr=tunnel annotation: kubectl annotate service my-service \"kube-router.io/service.dsr=tunnel\" Things To Lookout For In the current implementation, DSR will only be available to the external IPs or LoadBalancer IPs The current implementation does not support port remapping. So you need to use same port and target port for the service. In order for DSR to work correctly, an ipip tunnel to the pod is used. This reduces the MTU for the packet by 20 bytes. Because of the way DSR works it is not possible for clients to use PMTU to discover this MTU reduction. In TCP based services, we mitigate this by using iptables to set the TCP MSS value to 20 bytes less than kube-router's primary interface MTU size. However, it is not possible to do this for UDP streams. Therefore, UDP streams that continuously use large packets may see a performance impact due to packet fragmentation. Additionally, if clients set the DF (Do Not Fragment) bit, services may see packet loss on UDP services. Kubernetes Pod Examples As mentioned previously, if kube-router is run as a Kubernetes deployment, there are a couple of things needed on the deployment. Below is an example of what is necessary to get going (this is NOT a full deployment, it is just meant to highlight the elements needed for DSR): apiVersion: apps/v1 kind: DaemonSet metadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-system spec: selector: matchLabels: k8s-app: kube-router tier: node template: metadata: labels: k8s-app: kube-router tier: node spec: hostNetwork: true hostIPC: true hostPID: true volumes: - name: run hostPath: path: /var/run/docker.sock - name: rt-tables hostPath: path: /etc/iproute2/rt_tables ... containers: - name: kube-router image: docker.io/cloudnativelabs/kube-router:latest ... volumeMounts: - name: run mountPath: /var/run/docker.sock readOnly: true - name: rt-tables mountPath: /etc/iproute2/rt_tables readOnly: false ... For an example manifest please look at the kube-router all features manifest with DSR requirements for Docker enabled. DSR with containerd or cri-o As of kube-router-1.2.X and later, kube-router's DSR mode now works with non-docker container runtimes. Officially only containerd has been tested, but this solution should work with cri-o as well. Most of what was said above also applies for non-docker container runtimes, however, there are some adjustments that you'll need to make: You'll need to let kube-router know what container runtime socket to use via the --runtime-endpoint CLI parameter If running kube-router as a Kubernetes deployment you'll need to make sure that you expose the correct socket via hostPath volume mount Here is an example kube-router daemonset manifest with just the changes needed to enable DSR with containerd (this is not a full manifest, it is just meant to highlight differences): apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-router spec: template: spec: ... volumes: - name: containerd-sock hostPath: path: /run/containerd/containerd.sock ... containers: - name: kube-router args: - --runtime-endpoint=unix:///run/containerd/containerd.sock ... volumeMounts: - name: containerd-sock mountPath: /run/containerd/containerd.sock readOnly: true ... More Details About DSR In order to facilitate troubleshooting it is worth while to explain how kube-router accomplishes DSR functionality. kube-router adds iptables rules to the mangle table which marks incoming packets destined for DSR based services with a unique FW mark. This mark is then used in later stages to identify the packet and route it correctly. Additionally, for TCP streams, there are rules that enable TCP MSS since the packets will change MTU when traversing an ipip tunnel later on. kube-router adds the marks to an ip rule (see: ip-rule(8) ). This ip rule then forces the incoming DSR service packets to use a specific routing table. kube-router adds a new ip route table (at the time of this writing the table number is 78 ) which forces the packet to route to the host even though there are no interfaces on the host that carry the DSR IP address kube-router adds an IPVS server configured for the custom FW mark. When packets arrive on the localhost interface because of the above ip rule and ip route , IPVS will intercept them based on their unique FW mark. When pods selected by the DSR service become ready, kube-router adds endpoints configured for tunnel mode to the above IPVS server. Each endpoint is configured in tunnel mode (as opposed to masquerade mode), which then encapsulates the incoming packet in an ipip packet. It is at this point that the pod's destination IP is placed on the ipip packet header so that a packet can be routed to the pod via the kube-bridge on either this host or the destination host. kube-router then finds the targeted pod and enters it's local network namespace. Once inside the pod's linux network namespace, it sets up two new interfaces called kube-dummy-if and ipip . kube-dummy-if is configured with the externalIP address of the service. When the ipip packet arrives inside the pod, the original source packet with the externalIP is then extracted from the ipip packet via the ipip interface and is accepted to the listening application via the kube-dummy-if interface. When the application sends its response back to the client, it responds to the client's public IP address (since that is what it saw on the request's IP header) and the packet is returned directly to the client (as opposed to traversing the Kubernetes internal network and potentially making multiple intermediate hops)","title":"DSR"},{"location":"dsr/#direct-server-return","text":"","title":"Direct Server Return"},{"location":"dsr/#more-information","text":"For a more detailed explanation on how to use Direct Server Return (DSR) to build a highly scalable and available ingress for Kubernetes see the following blog post","title":"More Information"},{"location":"dsr/#what-is-dsr","text":"When enabled, DSR allows the service endpoint to respond directly to the client request, bypassing the service proxy. When DSR is enabled kube-router will use LVS's tunneling mode to achieve this (more on how later).","title":"What is DSR?"},{"location":"dsr/#quick-start","text":"You can enable DSR functionality on a per service basis. Requirements: ClusterIP type service has an externalIP set on it or is a LoadBalancer type service kube-router has been started with --service-external-ip-range configured at least once. This option can be specified multiple times for multiple ranges. The external IPs or LoadBalancer IPs must be included in these ranges. kube-router must be run in service proxy mode with --run-service-proxy (this option is defaulted to true if left unspecified) If you are advertising the service outside the cluster --advertise-external-ip must be set If kube-router is deployed as a Kubernetes pod: hostIPC: true must be set for the pod hostPID: true must be set for the pod The container runtime socket must be mounted into the kube-router pod via a hostPath volume mount. /etc/iproute2/rt_tables (or similar) must be read/write mounted into the kube-router pod via a hostPath volume mount. NOTE: since v6.5.0 of iproute2 this file has been moved underneath /usr in either /usr/lib/iproute2/rt_tables or /usr/share/iproute2/rt_tables instead of in /etc so this mount may need to be updated depending on which version of Linux you're deploying against. kube-router will check all 3 locations and use them in order of the above. A pod network that allows for IPIP encapsulated traffic. The most notable exception to this is that Azure does not transit IPIP encapsulated packets on their network. In this scenario, the end-user may be able to get around this issue by enabling FoU ( --overlay-encap=fou ) and full overlay networking ( --overlay-type=full ) options in kube-router. This hasn't been well tested, but it should allow the DSR encapsulated traffic to route correctly. To enable DSR you need to annotate service with the kube-router.io/service.dsr=tunnel annotation: kubectl annotate service my-service \"kube-router.io/service.dsr=tunnel\"","title":"Quick Start"},{"location":"dsr/#things-to-lookout-for","text":"In the current implementation, DSR will only be available to the external IPs or LoadBalancer IPs The current implementation does not support port remapping. So you need to use same port and target port for the service. In order for DSR to work correctly, an ipip tunnel to the pod is used. This reduces the MTU for the packet by 20 bytes. Because of the way DSR works it is not possible for clients to use PMTU to discover this MTU reduction. In TCP based services, we mitigate this by using iptables to set the TCP MSS value to 20 bytes less than kube-router's primary interface MTU size. However, it is not possible to do this for UDP streams. Therefore, UDP streams that continuously use large packets may see a performance impact due to packet fragmentation. Additionally, if clients set the DF (Do Not Fragment) bit, services may see packet loss on UDP services.","title":"Things To Lookout For"},{"location":"dsr/#kubernetes-pod-examples","text":"As mentioned previously, if kube-router is run as a Kubernetes deployment, there are a couple of things needed on the deployment. Below is an example of what is necessary to get going (this is NOT a full deployment, it is just meant to highlight the elements needed for DSR): apiVersion: apps/v1 kind: DaemonSet metadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-system spec: selector: matchLabels: k8s-app: kube-router tier: node template: metadata: labels: k8s-app: kube-router tier: node spec: hostNetwork: true hostIPC: true hostPID: true volumes: - name: run hostPath: path: /var/run/docker.sock - name: rt-tables hostPath: path: /etc/iproute2/rt_tables ... containers: - name: kube-router image: docker.io/cloudnativelabs/kube-router:latest ... volumeMounts: - name: run mountPath: /var/run/docker.sock readOnly: true - name: rt-tables mountPath: /etc/iproute2/rt_tables readOnly: false ... For an example manifest please look at the kube-router all features manifest with DSR requirements for Docker enabled.","title":"Kubernetes Pod Examples"},{"location":"dsr/#dsr-with-containerd-or-cri-o","text":"As of kube-router-1.2.X and later, kube-router's DSR mode now works with non-docker container runtimes. Officially only containerd has been tested, but this solution should work with cri-o as well. Most of what was said above also applies for non-docker container runtimes, however, there are some adjustments that you'll need to make: You'll need to let kube-router know what container runtime socket to use via the --runtime-endpoint CLI parameter If running kube-router as a Kubernetes deployment you'll need to make sure that you expose the correct socket via hostPath volume mount Here is an example kube-router daemonset manifest with just the changes needed to enable DSR with containerd (this is not a full manifest, it is just meant to highlight differences): apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-router spec: template: spec: ... volumes: - name: containerd-sock hostPath: path: /run/containerd/containerd.sock ... containers: - name: kube-router args: - --runtime-endpoint=unix:///run/containerd/containerd.sock ... volumeMounts: - name: containerd-sock mountPath: /run/containerd/containerd.sock readOnly: true ...","title":"DSR with containerd or cri-o"},{"location":"dsr/#more-details-about-dsr","text":"In order to facilitate troubleshooting it is worth while to explain how kube-router accomplishes DSR functionality. kube-router adds iptables rules to the mangle table which marks incoming packets destined for DSR based services with a unique FW mark. This mark is then used in later stages to identify the packet and route it correctly. Additionally, for TCP streams, there are rules that enable TCP MSS since the packets will change MTU when traversing an ipip tunnel later on. kube-router adds the marks to an ip rule (see: ip-rule(8) ). This ip rule then forces the incoming DSR service packets to use a specific routing table. kube-router adds a new ip route table (at the time of this writing the table number is 78 ) which forces the packet to route to the host even though there are no interfaces on the host that carry the DSR IP address kube-router adds an IPVS server configured for the custom FW mark. When packets arrive on the localhost interface because of the above ip rule and ip route , IPVS will intercept them based on their unique FW mark. When pods selected by the DSR service become ready, kube-router adds endpoints configured for tunnel mode to the above IPVS server. Each endpoint is configured in tunnel mode (as opposed to masquerade mode), which then encapsulates the incoming packet in an ipip packet. It is at this point that the pod's destination IP is placed on the ipip packet header so that a packet can be routed to the pod via the kube-bridge on either this host or the destination host. kube-router then finds the targeted pod and enters it's local network namespace. Once inside the pod's linux network namespace, it sets up two new interfaces called kube-dummy-if and ipip . kube-dummy-if is configured with the externalIP address of the service. When the ipip packet arrives inside the pod, the original source packet with the externalIP is then extracted from the ipip packet via the ipip interface and is accepted to the listening application via the kube-dummy-if interface. When the application sends its response back to the client, it responds to the client's public IP address (since that is what it saw on the request's IP header) and the packet is returned directly to the client (as opposed to traversing the Kubernetes internal network and potentially making multiple intermediate hops)","title":"More Details About DSR"},{"location":"generic/","text":"Kube-router on generic clusters This guide is for running kube-router as the CNI network provider for on premise and/or bare metal clusters outside of a cloud provider's environment. It assumes the initial cluster is bootstrapped and a networking provider needs configuration. All pod networking CIDRs are allocated by kube-controller-manager. Kube-router provides service/pod networking, a network policy firewall, and a high performance IPVS/LVS based service proxy. The network policy firewall and service proxy are both optional but recommended. Configuring the Worker Nodes If you choose to run kube-router as daemonset, then both kube-apiserver and kubelet must be run with --allow-privileged=true option (see our example daemonsets for more information ) Ensure your Container Runtime is configured to point its CNI configuration directory to /etc/cni/net.d . This is the default location for both containerd and cri-o , but can be set specifically if needed: containerd CRI Configuration Here is what the default containerd CNI plugin configuration looks like as of the writing of this document. The default containerd configuration can be retrieved using: containerd config default [plugins] [plugins.\"io.containerd.grpc.v1.cri\".cni] bin_dir = \"/opt/cni/bin\" conf_dir = \"/etc/cni/net.d\" conf_template = \"\" ip_pref = \"\" max_conf_num = 1 cri-o CRI Configuration cri-o CRI configuration can be referenced via their documentation If a previous CNI provider (e.g. weave-net, calico, or flannel) was used, remove old configurations from /etc/cni/net.d on each kubelet. Note: Switching CNI providers on a running cluster requires re-creating all pods to pick up new pod IPs** Configuring kube-controller-manager If you choose to use kube-router for pod-to-pod network connectivity then kube-controller-manager needs to be configured to allocate pod CIDRs by passing the --allocate-node-cidrs=true flag and providing a cluster-cidr (e.g. by passing --cluster-cidr=10.32.0.0/12 ) For example: --allocate-node-cidrs=true --cluster-cidr=10.32.0.0/12 --service-cluster-ip-range=10.50.0.0/22 Running kube-router with Everything This runs kube-router with pod/service networking, the network policy firewall, and service proxy to replace kube-proxy. The example command uses 10.32.0.0/12 as the pod CIDR address range and https://cluster01.int.domain.com:6443 as the apiserver address. Please change these to suit your cluster. CLUSTERCIDR=10.32.0.0/12 \\ APISERVER=https://cluster01.int.domain.com:6443 \\ sh -c 'curl -s https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml | \\ sed -e \"s;%APISERVER%;$APISERVER;g\" -e \"s;%CLUSTERCIDR%;$CLUSTERCIDR;g\"' | \\ kubectl apply -f - Removing a Previous kube-proxy If kube-proxy was ever deployed to the cluster, then you need to remove it when running kube-router in this capacity or they will conflict with each other. Remove any previously running kube-proxy and all iptables rules it created. Start by deleting the kube-proxy daemonset: kubectl -n kube-system delete ds kube-proxy Any iptables rules kube-proxy left around will also need to be cleaned up. This command might differ based on how kube-proxy was setup or configured: To cleanup kube-proxy we can do this with docker, containerd, or cri-o: docker docker run --privileged -v /lib/modules:/lib/modules --net=host registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy --cleanup containerd ctr images pull k8s.gcr.io/kube-proxy-amd64:v1.28.2 ctr run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro \\ registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy-cleanup kube-proxy --cleanup cri-o crictl pull registry.k8s.io/kube-proxy-amd64:v1.28.2 crictl run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy-cleanup kube-proxy --cleanup Running kube-router without the service proxy This runs kube-router with pod/service networking and the network policy firewall. The Service proxy is disabled. kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter.yaml In this mode kube-router relies on kube-proxy (or some other network service provider) to provide service networking. When service proxy is disabled kube-router will use in-cluster configuration to access APIserver through cluster-ip. Service networking must therefore be setup before deploying kube-router. Debugging kube-router supports setting log level via the command line -v or --v, To get maximal debug output from kube-router please start with --v=3","title":"generic"},{"location":"generic/#kube-router-on-generic-clusters","text":"This guide is for running kube-router as the CNI network provider for on premise and/or bare metal clusters outside of a cloud provider's environment. It assumes the initial cluster is bootstrapped and a networking provider needs configuration. All pod networking CIDRs are allocated by kube-controller-manager. Kube-router provides service/pod networking, a network policy firewall, and a high performance IPVS/LVS based service proxy. The network policy firewall and service proxy are both optional but recommended.","title":"Kube-router on generic clusters"},{"location":"generic/#configuring-the-worker-nodes","text":"If you choose to run kube-router as daemonset, then both kube-apiserver and kubelet must be run with --allow-privileged=true option (see our example daemonsets for more information ) Ensure your Container Runtime is configured to point its CNI configuration directory to /etc/cni/net.d . This is the default location for both containerd and cri-o , but can be set specifically if needed:","title":"Configuring the Worker Nodes"},{"location":"generic/#containerd-cri-configuration","text":"Here is what the default containerd CNI plugin configuration looks like as of the writing of this document. The default containerd configuration can be retrieved using: containerd config default [plugins] [plugins.\"io.containerd.grpc.v1.cri\".cni] bin_dir = \"/opt/cni/bin\" conf_dir = \"/etc/cni/net.d\" conf_template = \"\" ip_pref = \"\" max_conf_num = 1","title":"containerd CRI Configuration"},{"location":"generic/#cri-o-cri-configuration","text":"cri-o CRI configuration can be referenced via their documentation If a previous CNI provider (e.g. weave-net, calico, or flannel) was used, remove old configurations from /etc/cni/net.d on each kubelet.","title":"cri-o CRI Configuration"},{"location":"generic/#note-switching-cni-providers-on-a-running-cluster-requires-re-creating-all-pods-to-pick-up-new-pod-ips","text":"","title":"Note: Switching CNI providers on a running cluster requires re-creating all pods to pick up new pod IPs**"},{"location":"generic/#configuring-kube-controller-manager","text":"If you choose to use kube-router for pod-to-pod network connectivity then kube-controller-manager needs to be configured to allocate pod CIDRs by passing the --allocate-node-cidrs=true flag and providing a cluster-cidr (e.g. by passing --cluster-cidr=10.32.0.0/12 ) For example: --allocate-node-cidrs=true --cluster-cidr=10.32.0.0/12 --service-cluster-ip-range=10.50.0.0/22","title":"Configuring kube-controller-manager"},{"location":"generic/#running-kube-router-with-everything","text":"This runs kube-router with pod/service networking, the network policy firewall, and service proxy to replace kube-proxy. The example command uses 10.32.0.0/12 as the pod CIDR address range and https://cluster01.int.domain.com:6443 as the apiserver address. Please change these to suit your cluster. CLUSTERCIDR=10.32.0.0/12 \\ APISERVER=https://cluster01.int.domain.com:6443 \\ sh -c 'curl -s https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml | \\ sed -e \"s;%APISERVER%;$APISERVER;g\" -e \"s;%CLUSTERCIDR%;$CLUSTERCIDR;g\"' | \\ kubectl apply -f -","title":"Running kube-router with Everything"},{"location":"generic/#removing-a-previous-kube-proxy","text":"If kube-proxy was ever deployed to the cluster, then you need to remove it when running kube-router in this capacity or they will conflict with each other. Remove any previously running kube-proxy and all iptables rules it created. Start by deleting the kube-proxy daemonset: kubectl -n kube-system delete ds kube-proxy Any iptables rules kube-proxy left around will also need to be cleaned up. This command might differ based on how kube-proxy was setup or configured: To cleanup kube-proxy we can do this with docker, containerd, or cri-o:","title":"Removing a Previous kube-proxy"},{"location":"generic/#docker","text":"docker run --privileged -v /lib/modules:/lib/modules --net=host registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy --cleanup","title":"docker"},{"location":"generic/#containerd","text":"ctr images pull k8s.gcr.io/kube-proxy-amd64:v1.28.2 ctr run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro \\ registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy-cleanup kube-proxy --cleanup","title":"containerd"},{"location":"generic/#cri-o","text":"crictl pull registry.k8s.io/kube-proxy-amd64:v1.28.2 crictl run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy-cleanup kube-proxy --cleanup","title":"cri-o"},{"location":"generic/#running-kube-router-without-the-service-proxy","text":"This runs kube-router with pod/service networking and the network policy firewall. The Service proxy is disabled. kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter.yaml In this mode kube-router relies on kube-proxy (or some other network service provider) to provide service networking. When service proxy is disabled kube-router will use in-cluster configuration to access APIserver through cluster-ip. Service networking must therefore be setup before deploying kube-router.","title":"Running kube-router without the service proxy"},{"location":"generic/#debugging","text":"kube-router supports setting log level via the command line -v or --v, To get maximal debug output from kube-router please start with --v=3","title":"Debugging"},{"location":"health/","text":"Health checking kube-router kube-router currently has basic health checking in form of heartbeats sent from each controller to the healthcontroller each time the main loop completes successfully. The health port is by default 20244 but can be changed with the startup option. The health path is /healthz --health-port=<port number> If port is set to 0 (zero) no HTTP endpoint will be made available but the health controller will still run and print out any missed heartbeats to STDERR of kube-router If a controller does not send a heartbeat within controllersynctime + 5 seconds the component will be flagged as unhealthy. If any of the running components is failing the whole kube-router state will be marked as failed in the /healthz endpoint For example, if kube-router is started with --run-router=true --run-firewall=true --run-service-proxy=true --run-loadbalancer=true If the route controller, policy controller or service controller exits it's main loop and does not publish a heartbeat the /healthz endpoint will return a error 500 signaling that kube-router is not healthy.","title":"Health"},{"location":"health/#health-checking-kube-router","text":"kube-router currently has basic health checking in form of heartbeats sent from each controller to the healthcontroller each time the main loop completes successfully. The health port is by default 20244 but can be changed with the startup option. The health path is /healthz --health-port=<port number> If port is set to 0 (zero) no HTTP endpoint will be made available but the health controller will still run and print out any missed heartbeats to STDERR of kube-router If a controller does not send a heartbeat within controllersynctime + 5 seconds the component will be flagged as unhealthy. If any of the running components is failing the whole kube-router state will be marked as failed in the /healthz endpoint For example, if kube-router is started with --run-router=true --run-firewall=true --run-service-proxy=true --run-loadbalancer=true If the route controller, policy controller or service controller exits it's main loop and does not publish a heartbeat the /healthz endpoint will return a error 500 signaling that kube-router is not healthy.","title":"Health checking kube-router"},{"location":"how-it-works/","text":"Theory of Operation Kube-router can be run as an agent or a Pod (via DaemonSet) on each node and leverages standard Linux technologies iptables, ipvs/lvs, ipset, iproute2 Service Proxy And Load Balancing Blog: Kubernetes network services proxy with IPVS/LVS Kube-router uses IPVS/LVS technology built in Linux to provide L4 load balancing. Each ClusterIP , NodePort , and LoadBalancer Kubernetes Service type is configured as an IPVS virtual service. Each Service Endpoint is configured as real server to the virtual service. The standard ipvsadm tool can be used to verify the configuration and monitor the active connections. Below is example set of Services on Kubernetes: and the Endpoints for the Services: and how they got mapped to the IPVS by kube-router: Kube-router watches the Kubernetes API server to get updates on the Services/Endpoints and automatically syncs the IPVS configuration to reflect the desired state of Services. Kube-router uses IPVS masquerading mode and uses round robin scheduling currently. Source pod IP is preserved so that appropriate network policies can be applied. Pod Ingress Firewall Blog: Enforcing Kubernetes network policies with iptables Kube-router provides an implementation of Kubernetes Network Policies through the use of iptables, ipset and conntrack. All the Pods in a Namespace with 'DefaultDeny' ingress isolation policy has ingress blocked. Only traffic that matches whitelist rules specified in the network policies are permitted to reach those Pods. The following set of iptables rules and chains in the 'filter' table are used to achieve the Network Policies semantics. Each Pod running on the Node which needs ingress blocked by default is matched in FORWARD and OUTPUT chains of the fliter table and are sent to a pod specific firewall chain. Below rules are added to match various cases Traffic getting switched between the Pods on the same Node through the local bridge Traffic getting routed between the Pods on different Nodes Traffic originating from a Pod and going through the Service proxy and getting routed to a Pod on the same Node Each Pod specific firewall chain has default rule to block the traffic. Rules are added to jump traffic to the Network Policy specific policy chains. Rules cover only policies that apply to the destination pod ip. A rule is added to accept the the established traffic to permit the return traffic. Each policy chain has rules expressed through source and destination ipsets. Set of pods matching ingress rule in network policy spec forms a source Pod ip ipset. set of Pods matching pod selector (for destination Pods) in the Network Policy forms destination Pod ip ipset. Finally ipsets are created that are used in forming the rules in the Network Policy specific chain Kube-router at runtime watches Kubernetes API server for changes in the namespace, network policy and pods and dynamically updates iptables and ipset configuration to reflect desired state of ingress firewall for the the pods. Pod Networking Blog: Kubernetes pod networking and beyond with BGP Kube-router is expected to run on each Node. The subnet of the Node is obtained from the CNI configuration file on the Node or through the Node.PodCidr. Each kube-router instance on the Node acts as a BGP router and advertises the Pod CIDR assigned to the Node. Each Node peers with rest of the Nodes in the cluster forming full mesh. Learned routes about the Pod CIDR from the other Nodes (BGP peers) are injected into local Node routing table. On the data path, inter Node Pod-to-Pod communication is done by the routing stack on the Node.","title":"How it works"},{"location":"how-it-works/#theory-of-operation","text":"Kube-router can be run as an agent or a Pod (via DaemonSet) on each node and leverages standard Linux technologies iptables, ipvs/lvs, ipset, iproute2","title":"Theory of Operation"},{"location":"how-it-works/#service-proxy-and-load-balancing","text":"Blog: Kubernetes network services proxy with IPVS/LVS Kube-router uses IPVS/LVS technology built in Linux to provide L4 load balancing. Each ClusterIP , NodePort , and LoadBalancer Kubernetes Service type is configured as an IPVS virtual service. Each Service Endpoint is configured as real server to the virtual service. The standard ipvsadm tool can be used to verify the configuration and monitor the active connections. Below is example set of Services on Kubernetes: and the Endpoints for the Services: and how they got mapped to the IPVS by kube-router: Kube-router watches the Kubernetes API server to get updates on the Services/Endpoints and automatically syncs the IPVS configuration to reflect the desired state of Services. Kube-router uses IPVS masquerading mode and uses round robin scheduling currently. Source pod IP is preserved so that appropriate network policies can be applied.","title":"Service Proxy And Load Balancing"},{"location":"how-it-works/#pod-ingress-firewall","text":"Blog: Enforcing Kubernetes network policies with iptables Kube-router provides an implementation of Kubernetes Network Policies through the use of iptables, ipset and conntrack. All the Pods in a Namespace with 'DefaultDeny' ingress isolation policy has ingress blocked. Only traffic that matches whitelist rules specified in the network policies are permitted to reach those Pods. The following set of iptables rules and chains in the 'filter' table are used to achieve the Network Policies semantics. Each Pod running on the Node which needs ingress blocked by default is matched in FORWARD and OUTPUT chains of the fliter table and are sent to a pod specific firewall chain. Below rules are added to match various cases Traffic getting switched between the Pods on the same Node through the local bridge Traffic getting routed between the Pods on different Nodes Traffic originating from a Pod and going through the Service proxy and getting routed to a Pod on the same Node Each Pod specific firewall chain has default rule to block the traffic. Rules are added to jump traffic to the Network Policy specific policy chains. Rules cover only policies that apply to the destination pod ip. A rule is added to accept the the established traffic to permit the return traffic. Each policy chain has rules expressed through source and destination ipsets. Set of pods matching ingress rule in network policy spec forms a source Pod ip ipset. set of Pods matching pod selector (for destination Pods) in the Network Policy forms destination Pod ip ipset. Finally ipsets are created that are used in forming the rules in the Network Policy specific chain Kube-router at runtime watches Kubernetes API server for changes in the namespace, network policy and pods and dynamically updates iptables and ipset configuration to reflect desired state of ingress firewall for the the pods.","title":"Pod Ingress Firewall"},{"location":"how-it-works/#pod-networking","text":"Blog: Kubernetes pod networking and beyond with BGP Kube-router is expected to run on each Node. The subnet of the Node is obtained from the CNI configuration file on the Node or through the Node.PodCidr. Each kube-router instance on the Node acts as a BGP router and advertises the Pod CIDR assigned to the Node. Each Node peers with rest of the Nodes in the cluster forming full mesh. Learned routes about the Pod CIDR from the other Nodes (BGP peers) are injected into local Node routing table. On the data path, inter Node Pod-to-Pod communication is done by the routing stack on the Node.","title":"Pod Networking"},{"location":"introduction/","text":"Introduction Welcome to the introduction guide to Kube-router! This guide is the best place to start with Kube-router. We cover what kube-router is, what problems it can solve, how it compares to existing software, and how you can get started using it. If you are familiar with the basics of Kube-router, head over to the next sections that provide a more detailed reference of available features. What is Kube-router If you are not familiar with Kubernetes networking model it is recommended to familiarize with Kubernetes networking model . So essentially Kubernetes expects: all containers can communicate with all other containers without NAT all nodes can communicate with all containers (and vice-versa) without NAT the IP that a container sees itself as is the same IP that others see it as Kubernetes only prescribes the requirements for the networking model but does not provide any default implementation. For a functional Kubernetes cluster one has to deploy what is called as CNI or pod networking solution that provides the above functionality. Any non-trivial containerized application will end up running multiple pods and exposing different services. Service abstraction in Kubernetes is an essential building block that helps in service discovery and load balancing. A layer-4 service proxy must be deployed to the Kubernetes cluster that provides the load-balancing for the services exposed by the pods. Once you have pod-to-pod networking established and have a service proxy that provides load-balancing, you need a way to secure your pods. Kubernetes Network Policies provide a specfication to secure pods. You need to deploy a solution that implements network policy specification and provides security for your pods. If you utilize LoadBalancer services in your cluster, then you need to deploy a solution that will allocate and manage your LoadBalancer IP address space. Kube-router is a turnkey solution for Kubernetes networking that provides all the above essential functionality in one single elegant package. Why Kube-router Network is hard. You have multiple Kubernetes networking solutions that provide pod networking or network policy etc. But when you deploy indiviudal solution for each functionality you end up with lot of moving parts making it difficult to operate and troubleshoot. Kube-router is a lean yet powerful all-in-one alternative to several network components used in typical Kubernetes clusters. All this from a single DaemonSet/Binary. It doesn't get any easier. Kube-router also uses best-of-breed low-level kernel solutions for maximum performance. Kube-router uses IPVS/LVS for service proxying and provides direct routing between the nodes. Kube-router also provides very unique and advanced functionalities like DSR (Direct Server Return), ECMP based network load balancing etc.","title":"Introduction"},{"location":"introduction/#introduction","text":"Welcome to the introduction guide to Kube-router! This guide is the best place to start with Kube-router. We cover what kube-router is, what problems it can solve, how it compares to existing software, and how you can get started using it. If you are familiar with the basics of Kube-router, head over to the next sections that provide a more detailed reference of available features.","title":"Introduction"},{"location":"introduction/#what-is-kube-router","text":"If you are not familiar with Kubernetes networking model it is recommended to familiarize with Kubernetes networking model . So essentially Kubernetes expects: all containers can communicate with all other containers without NAT all nodes can communicate with all containers (and vice-versa) without NAT the IP that a container sees itself as is the same IP that others see it as Kubernetes only prescribes the requirements for the networking model but does not provide any default implementation. For a functional Kubernetes cluster one has to deploy what is called as CNI or pod networking solution that provides the above functionality. Any non-trivial containerized application will end up running multiple pods and exposing different services. Service abstraction in Kubernetes is an essential building block that helps in service discovery and load balancing. A layer-4 service proxy must be deployed to the Kubernetes cluster that provides the load-balancing for the services exposed by the pods. Once you have pod-to-pod networking established and have a service proxy that provides load-balancing, you need a way to secure your pods. Kubernetes Network Policies provide a specfication to secure pods. You need to deploy a solution that implements network policy specification and provides security for your pods. If you utilize LoadBalancer services in your cluster, then you need to deploy a solution that will allocate and manage your LoadBalancer IP address space. Kube-router is a turnkey solution for Kubernetes networking that provides all the above essential functionality in one single elegant package.","title":"What is Kube-router"},{"location":"introduction/#why-kube-router","text":"Network is hard. You have multiple Kubernetes networking solutions that provide pod networking or network policy etc. But when you deploy indiviudal solution for each functionality you end up with lot of moving parts making it difficult to operate and troubleshoot. Kube-router is a lean yet powerful all-in-one alternative to several network components used in typical Kubernetes clusters. All this from a single DaemonSet/Binary. It doesn't get any easier. Kube-router also uses best-of-breed low-level kernel solutions for maximum performance. Kube-router uses IPVS/LVS for service proxying and provides direct routing between the nodes. Kube-router also provides very unique and advanced functionalities like DSR (Direct Server Return), ECMP based network load balancing etc.","title":"Why Kube-router"},{"location":"ipv6/","text":"IPv6 / Dual-Stack Support in kube-router This document describes the current status, the plan ahead and general thoughts about IPv6 / Dual-Stack support in kube-router. Dual-Stack (e.g. IPv4 and IPv6) has been supported in Kubernetes since version v1.21 : IPv4/IPv6 dual-stack documentation kube-router's current approach is to implement dual-stack functionality function-by-function: CNI --enable-cni Proxy --run-service-proxy Router --run-router Network policies --run-firewall Current status (Oct 7, 2023) Support for dual-stack in kube-router is feature complete. Release v2.0.0 and above of kube-router has all controllers updated for dual-stack compatibility. Important Notes / Known Limitations / Etc This represents a major release for kube-router and as such, user's should approach deploying this into an established kube-router environment carefully. While there aren't any huge bugs that the maintainers are aware of at this time, there are several small breaks in backwards compatibility. We'll try to detail these below as best we can. How To Enable Dual-Stack Functionality In order to enable dual-stack functionality please ensure the following: kube-router option --enable-ipv4=true is set (this is the default) kube-router option --enable-ipv6=true is set Your Kubernetes node has both IPv4 and IPv6 addresses on its physical interfaces Your Kubernetes node has both IPv4 and IPv6 addresses in its node spec: $ kubectl describe node foo ... Addresses: InternalIP: 10.95.0.202 InternalIP: 2001:1f18:3d5:ed00:d61a:454f:b886:7000 Hostname: foo ... Add additional --service-cluster-ip-range and --service-external-ip-range kube-router parameters for your IPv6 addresses. If you use --enable-cni=true , ensure kube-controller-manager has been started with both IPv4 and IPv6 cluster CIDRs (e.g. --cluster-cidr=10.242.0.0/16,2001:db8:42:1000::/56 ) Ensure kube-controller-manager & kube-apiserver have been started with both IPv4 and IPv6 service cluster IP ranges (e.g. --service-cluster-ip-range=10.96.0.0/16,2001:db8:42:1::/112 ) Tunnel Name Changes (Potentially Breaking Change) In order to facilitate both IPv4 and IPv6 tunnels, we had to change the hashing format for our current tunnel names. As such, if you do a kube-router upgrade in place (i.e. without reboot), it is very likely that kube-router will not clean up old tunnels. This will only impact users that were utilizing the overlay feature of kube-router to some extent. Such as if you were running kube-router with --enable-overlay or --overlay-type=full or --overlay-type=subnet (it should be noted that these options default to on currently). If you are upgrading kube-router from a pre v2.0.0 release to a v2.0.0 release, we recommend that you coordinate your upgrade of kube-router with a rolling reboot of your Kubernetes fleet to clean up any tunnels that were left from previous versions of kube-router. Differences in --override-nexthop While v2.X and above versions of kube-router are IPv6 compatible and advertise both IPv4 and IPv6 addresses, it still does this over a single BGP peering. This peering is made from what kube-router considers the node's primary IP address. Which is typically the first internal IP address listed in the node's Kubernetes metadata (e.g. kubectl get node ) unless it is overriden by a local-address annotation configuration. This address with be either an IPv4 or IPv6 address and kube-router will use this to make the peering. Without --override-nexthop kube-router does the work to ensure that an IP or subnet is advertised by the matching IP family for the IP or subnet. However, with --override-nexthop enabled kube-router doesn't have control over what the next-hop for the advertised route will be. Instead the next-hop will be overridden by the IP that is being used to peer with kube-router. This can cause trouble for many configurations and so it is not recommended to use --override-nexthop in dual-stack kube-router configurations. One place where this was particularly problematic was when advertising the Pod IP subnets between different kube-router enabled Kubernetes worker nodes. Workers that use overlay networking in a kube-router cluster are made aware of their neighbors via BGP protocol advertisements and --override-nexthop would mean that one family of addresses would never work correctly. As such, we no longer apply the --override-nexthop setting to pod subnet advertisements between kube-router nodes. This is different functionality between version v1.X of kube-router and v2.x. kube-router.io/node.bgp.customimportreject Can Only Contain IPs of a Single Family Due to implementation restrictions with GoBGP, the annotation kube-router.io/node.bgp.customimportreject , which allows user's to add rules for rejecting specific routes sent to GoBGP, can only accept a single IP family (e.g. IPv4 or IPv6). Attempting to add IPs of two different families will result in a GoBGP error when it attempts to import BGP policy from kube-router. IPv6 & IPv4 Network Policy Ranges Will Only Work If That Family Has Been Enabled Network Policy in Kubernetes allows users to specify IPBlock ranges for ingress and egress policies. These blocks are string-based network CIDRs and allow the user to specify any ranges that they wish in order to allow ingress or egress from network ranges that are not selectable using Kubernetes pod selectors. Currently, kube-router is only able to work with CIDRs for IP families that it has been enabled for using the --enable-ipv4=true & --enable-ipv6=true CLI flags. If a user adds a network policy for an IP family that kube-router is not enabled for, you will see a warning in your kube-router logs and no firewall rule will be added. kube-router.io/pod-cidr Deprecation Now that kube-router has dual-stack capability, it doesn't make sense to have an annotation that can only represent a single pod CIDR any longer. As such, with this release we are announcing the deprecation of the kube-router.io/pod-cidr annotation in favor of the new kube-router.io/pod-cidrs annotation. The new kube-router.io/pod-cidrs annotation is a comma-separated list of CIDRs and can hold either IPv4 or IPv6 CIDRs in string form. It should be noted, that until kube-router.io/pod-cidr is fully removed, at some point in the future, it will still be preferred over the kube-router.io/pod-cidrs annotation in order to preserve as much backwards compatibility as possible. Until kube-router.io/pod-cidr has been fully retired, users that use the old annotation will get a warning in their kube-router logs saying that they should change to the new annotation. The recommended action here, is that upon upgrade, you convert nodes from using the kube-router.io/pod-cidr to the new kube-router.io/pod-cidrs annotation. Since kube-router currently only updates node annotations at start and not as they are updated, this is a safe change to make before updating kube-router. If neither annotation is specified, kube-router will use the PodCIDRs field of the Kubernetes node spec which is populated by the kube-controller-manager as part of it's --allocate-node-cidrs functionality. This should be a sane default for most users of kube-router. CNI Now Accepts Multiple Pod Ranges Now that kube-router supports dual-stack, it also supports multiple ranges in the CNI file. While kube-router will still add your pod CIDRs to your CNI configuration via node configuration like kube-router.io/pod-cidr , kube-router.io/pod-cidrs , or .node.Spec.PodCIDRs , you can also customize your own CNI to add additional ranges or plugins. A CNI configuration with multiple ranges will typically look something like the following: { \"cniVersion\": \"0.3.0\", \"name\": \"mynet\", \"plugins\": [ { \"bridge\": \"kube-bridge\", \"ipam\": { \"ranges\": [ [ { \"subnet\": \"10.242.0.0/24\" } ], [ { \"subnet\": \"2001:db8:42:1000::/64\" } ] ], \"type\": \"host-local\" }, \"isDefaultGateway\": true, \"mtu\": 9001, \"name\": \"kubernetes\", \"type\": \"bridge\" } ] } All kube-router's handling of the CNI file attempts to minimize disruption to any user made edits to the file.","title":"IPv6"},{"location":"ipv6/#ipv6-dual-stack-support-in-kube-router","text":"This document describes the current status, the plan ahead and general thoughts about IPv6 / Dual-Stack support in kube-router. Dual-Stack (e.g. IPv4 and IPv6) has been supported in Kubernetes since version v1.21 : IPv4/IPv6 dual-stack documentation kube-router's current approach is to implement dual-stack functionality function-by-function: CNI --enable-cni Proxy --run-service-proxy Router --run-router Network policies --run-firewall","title":"IPv6 / Dual-Stack Support in kube-router"},{"location":"ipv6/#current-status-oct-7-2023","text":"Support for dual-stack in kube-router is feature complete. Release v2.0.0 and above of kube-router has all controllers updated for dual-stack compatibility.","title":"Current status (Oct 7, 2023)"},{"location":"ipv6/#important-notes-known-limitations-etc","text":"This represents a major release for kube-router and as such, user's should approach deploying this into an established kube-router environment carefully. While there aren't any huge bugs that the maintainers are aware of at this time, there are several small breaks in backwards compatibility. We'll try to detail these below as best we can.","title":"Important Notes / Known Limitations / Etc"},{"location":"ipv6/#how-to-enable-dual-stack-functionality","text":"In order to enable dual-stack functionality please ensure the following: kube-router option --enable-ipv4=true is set (this is the default) kube-router option --enable-ipv6=true is set Your Kubernetes node has both IPv4 and IPv6 addresses on its physical interfaces Your Kubernetes node has both IPv4 and IPv6 addresses in its node spec: $ kubectl describe node foo ... Addresses: InternalIP: 10.95.0.202 InternalIP: 2001:1f18:3d5:ed00:d61a:454f:b886:7000 Hostname: foo ... Add additional --service-cluster-ip-range and --service-external-ip-range kube-router parameters for your IPv6 addresses. If you use --enable-cni=true , ensure kube-controller-manager has been started with both IPv4 and IPv6 cluster CIDRs (e.g. --cluster-cidr=10.242.0.0/16,2001:db8:42:1000::/56 ) Ensure kube-controller-manager & kube-apiserver have been started with both IPv4 and IPv6 service cluster IP ranges (e.g. --service-cluster-ip-range=10.96.0.0/16,2001:db8:42:1::/112 )","title":"How To Enable Dual-Stack Functionality"},{"location":"ipv6/#tunnel-name-changes-potentially-breaking-change","text":"In order to facilitate both IPv4 and IPv6 tunnels, we had to change the hashing format for our current tunnel names. As such, if you do a kube-router upgrade in place (i.e. without reboot), it is very likely that kube-router will not clean up old tunnels. This will only impact users that were utilizing the overlay feature of kube-router to some extent. Such as if you were running kube-router with --enable-overlay or --overlay-type=full or --overlay-type=subnet (it should be noted that these options default to on currently). If you are upgrading kube-router from a pre v2.0.0 release to a v2.0.0 release, we recommend that you coordinate your upgrade of kube-router with a rolling reboot of your Kubernetes fleet to clean up any tunnels that were left from previous versions of kube-router.","title":"Tunnel Name Changes (Potentially Breaking Change)"},{"location":"ipv6/#differences-in-override-nexthop","text":"While v2.X and above versions of kube-router are IPv6 compatible and advertise both IPv4 and IPv6 addresses, it still does this over a single BGP peering. This peering is made from what kube-router considers the node's primary IP address. Which is typically the first internal IP address listed in the node's Kubernetes metadata (e.g. kubectl get node ) unless it is overriden by a local-address annotation configuration. This address with be either an IPv4 or IPv6 address and kube-router will use this to make the peering. Without --override-nexthop kube-router does the work to ensure that an IP or subnet is advertised by the matching IP family for the IP or subnet. However, with --override-nexthop enabled kube-router doesn't have control over what the next-hop for the advertised route will be. Instead the next-hop will be overridden by the IP that is being used to peer with kube-router. This can cause trouble for many configurations and so it is not recommended to use --override-nexthop in dual-stack kube-router configurations. One place where this was particularly problematic was when advertising the Pod IP subnets between different kube-router enabled Kubernetes worker nodes. Workers that use overlay networking in a kube-router cluster are made aware of their neighbors via BGP protocol advertisements and --override-nexthop would mean that one family of addresses would never work correctly. As such, we no longer apply the --override-nexthop setting to pod subnet advertisements between kube-router nodes. This is different functionality between version v1.X of kube-router and v2.x.","title":"Differences in --override-nexthop"},{"location":"ipv6/#kube-routerionodebgpcustomimportreject-can-only-contain-ips-of-a-single-family","text":"Due to implementation restrictions with GoBGP, the annotation kube-router.io/node.bgp.customimportreject , which allows user's to add rules for rejecting specific routes sent to GoBGP, can only accept a single IP family (e.g. IPv4 or IPv6). Attempting to add IPs of two different families will result in a GoBGP error when it attempts to import BGP policy from kube-router.","title":"kube-router.io/node.bgp.customimportreject Can Only Contain IPs of a Single Family"},{"location":"ipv6/#ipv6-ipv4-network-policy-ranges-will-only-work-if-that-family-has-been-enabled","text":"Network Policy in Kubernetes allows users to specify IPBlock ranges for ingress and egress policies. These blocks are string-based network CIDRs and allow the user to specify any ranges that they wish in order to allow ingress or egress from network ranges that are not selectable using Kubernetes pod selectors. Currently, kube-router is only able to work with CIDRs for IP families that it has been enabled for using the --enable-ipv4=true & --enable-ipv6=true CLI flags. If a user adds a network policy for an IP family that kube-router is not enabled for, you will see a warning in your kube-router logs and no firewall rule will be added.","title":"IPv6 &amp; IPv4 Network Policy Ranges Will Only Work If That Family Has Been Enabled"},{"location":"ipv6/#kube-routeriopod-cidr-deprecation","text":"Now that kube-router has dual-stack capability, it doesn't make sense to have an annotation that can only represent a single pod CIDR any longer. As such, with this release we are announcing the deprecation of the kube-router.io/pod-cidr annotation in favor of the new kube-router.io/pod-cidrs annotation. The new kube-router.io/pod-cidrs annotation is a comma-separated list of CIDRs and can hold either IPv4 or IPv6 CIDRs in string form. It should be noted, that until kube-router.io/pod-cidr is fully removed, at some point in the future, it will still be preferred over the kube-router.io/pod-cidrs annotation in order to preserve as much backwards compatibility as possible. Until kube-router.io/pod-cidr has been fully retired, users that use the old annotation will get a warning in their kube-router logs saying that they should change to the new annotation. The recommended action here, is that upon upgrade, you convert nodes from using the kube-router.io/pod-cidr to the new kube-router.io/pod-cidrs annotation. Since kube-router currently only updates node annotations at start and not as they are updated, this is a safe change to make before updating kube-router. If neither annotation is specified, kube-router will use the PodCIDRs field of the Kubernetes node spec which is populated by the kube-controller-manager as part of it's --allocate-node-cidrs functionality. This should be a sane default for most users of kube-router.","title":"kube-router.io/pod-cidr Deprecation"},{"location":"ipv6/#cni-now-accepts-multiple-pod-ranges","text":"Now that kube-router supports dual-stack, it also supports multiple ranges in the CNI file. While kube-router will still add your pod CIDRs to your CNI configuration via node configuration like kube-router.io/pod-cidr , kube-router.io/pod-cidrs , or .node.Spec.PodCIDRs , you can also customize your own CNI to add additional ranges or plugins. A CNI configuration with multiple ranges will typically look something like the following: { \"cniVersion\": \"0.3.0\", \"name\": \"mynet\", \"plugins\": [ { \"bridge\": \"kube-bridge\", \"ipam\": { \"ranges\": [ [ { \"subnet\": \"10.242.0.0/24\" } ], [ { \"subnet\": \"2001:db8:42:1000::/64\" } ] ], \"type\": \"host-local\" }, \"isDefaultGateway\": true, \"mtu\": 9001, \"name\": \"kubernetes\", \"type\": \"bridge\" } ] } All kube-router's handling of the CNI file attempts to minimize disruption to any user made edits to the file.","title":"CNI Now Accepts Multiple Pod Ranges"},{"location":"kops/","text":"Kops Integration Kops version 1.6.2 and above now officially includes an integration with kube-router. Please follow the instructions at their official documentation to provision a Kubernetes cluster with Kube-router. Uses the kops latest version binaries which has kube-router support.","title":"kops"},{"location":"kops/#kops-integration","text":"Kops version 1.6.2 and above now officially includes an integration with kube-router. Please follow the instructions at their official documentation to provision a Kubernetes cluster with Kube-router. Uses the kops latest version binaries which has kube-router support.","title":"Kops Integration"},{"location":"kubeadm/","text":"Deploying kube-router with kubeadm Please follow the steps to install Kubernetes cluster with Kubeadm, however must specify --pod-network-cidr when you run kubeadm init . kube-router relies on kube-controller-manager to allocate pod CIDR for the nodes. kube-router provides pod networking, network policy and high perfoming IPVS/LVS based service proxy. Depending on your choice to use kube-router for service proxy you have two options. kube-router Providing Pod Networking and Network Policy For the step #3 Installing a Pod network add-on install a kube-router pod network and network policy add-on with the following command: KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml kube-router Providing Service Proxy, Firewall and Pod Networking For the step #3 Installing a Pod network add-on install a kube-router pod network and network policy add-on with the following command: KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml Now since kube-router provides service proxy as well. Run below commands to remove kube-proxy and cleanup any iptables configuration it may have done. KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n kube-system delete ds kube-proxy To cleanup kube-proxy we can do this with docker, containerd, or cri-o: docker docker run --privileged -v /lib/modules:/lib/modules --net=host registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy --cleanup containerd ctr images pull k8s.gcr.io/kube-proxy-amd64:v1.28.2 ctr run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro \\ registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy-cleanup kube-proxy --cleanup cri-o crictl pull registry.k8s.io/kube-proxy-amd64:v1.28.2 crictl run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy-cleanup kube-proxy --cleanup","title":"kubeadm"},{"location":"kubeadm/#deploying-kube-router-with-kubeadm","text":"Please follow the steps to install Kubernetes cluster with Kubeadm, however must specify --pod-network-cidr when you run kubeadm init . kube-router relies on kube-controller-manager to allocate pod CIDR for the nodes. kube-router provides pod networking, network policy and high perfoming IPVS/LVS based service proxy. Depending on your choice to use kube-router for service proxy you have two options.","title":"Deploying kube-router with kubeadm"},{"location":"kubeadm/#kube-router-providing-pod-networking-and-network-policy","text":"For the step #3 Installing a Pod network add-on install a kube-router pod network and network policy add-on with the following command: KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml","title":"kube-router Providing Pod Networking and Network Policy"},{"location":"kubeadm/#kube-router-providing-service-proxy-firewall-and-pod-networking","text":"For the step #3 Installing a Pod network add-on install a kube-router pod network and network policy add-on with the following command: KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml Now since kube-router provides service proxy as well. Run below commands to remove kube-proxy and cleanup any iptables configuration it may have done. KUBECONFIG=/etc/kubernetes/admin.conf kubectl -n kube-system delete ds kube-proxy To cleanup kube-proxy we can do this with docker, containerd, or cri-o:","title":"kube-router Providing Service Proxy, Firewall and Pod Networking"},{"location":"kubeadm/#docker","text":"docker run --privileged -v /lib/modules:/lib/modules --net=host registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy --cleanup","title":"docker"},{"location":"kubeadm/#containerd","text":"ctr images pull k8s.gcr.io/kube-proxy-amd64:v1.28.2 ctr run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro \\ registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy-cleanup kube-proxy --cleanup","title":"containerd"},{"location":"kubeadm/#cri-o","text":"crictl pull registry.k8s.io/kube-proxy-amd64:v1.28.2 crictl run --rm --privileged --net-host --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro registry.k8s.io/kube-proxy-amd64:v1.28.2 kube-proxy-cleanup kube-proxy --cleanup","title":"cri-o"},{"location":"load-balancer-allocator/","text":"Load Balancer allocator What does it do The load balancer allocator controller looks for services with the type LoadBalancer and tries to allocate addresses for it if needed. The controller doesn't enable any announcement of the addresses by default, so --advertise-loadbalancer-ip should be set to true and BGP peers configured. Load balancer classes By default the controller allocates addresses for all LoadBalancer services with the where loadBalancerClass is empty or set to one of \"default\" or \"kube-router\". If --loadbalancer-default-class is set to false, the controller will only handle services with the class set to \"kube-router\". RBAC permissions The controller needs some extra permissions to get, create and update leases for leader election and to update services with allocated addresses. Example permissions: kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-router namespace: kube-system rules: - apiGroups: - \"coordination.k8s.io\" resources: - leases verbs: - get - create - update - apiGroups: - \"\" resources: - services/status verbs: - update Environment variables The controller uses the environment variable POD_NAME as the identify for the lease used for leader election. Using the kubernetes downward api to set POD_NAME to the pod name the lease identify will match the current leader. --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-system spec: ... template: metadata: .... spec: ... env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name ... The environment variable POD_NAMESPACE can also be specified to set the namespace used for the lease. By default the namespace is looked up from within the pod using /var/run/secrets/kubernetes.io/serviceaccount/namespace . Running outside kubernetes When running the controller outside a pod, both POD_NAME and POD_NAMESPACE must set for the controller to work. POD_NAME should be unique per instance, so using for example the hostname of the machine might be a good idea. POD_NAMESPACE must be the same across all instances running in the same cluster. Notes It's not possible to specify the addresses for the load balancer services. A externalIP service can be used instead.","title":"Load Balancer Allocator"},{"location":"load-balancer-allocator/#load-balancer-allocator","text":"","title":"Load Balancer allocator"},{"location":"load-balancer-allocator/#what-does-it-do","text":"The load balancer allocator controller looks for services with the type LoadBalancer and tries to allocate addresses for it if needed. The controller doesn't enable any announcement of the addresses by default, so --advertise-loadbalancer-ip should be set to true and BGP peers configured.","title":"What does it do"},{"location":"load-balancer-allocator/#load-balancer-classes","text":"By default the controller allocates addresses for all LoadBalancer services with the where loadBalancerClass is empty or set to one of \"default\" or \"kube-router\". If --loadbalancer-default-class is set to false, the controller will only handle services with the class set to \"kube-router\".","title":"Load balancer classes"},{"location":"load-balancer-allocator/#rbac-permissions","text":"The controller needs some extra permissions to get, create and update leases for leader election and to update services with allocated addresses. Example permissions: kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kube-router namespace: kube-system rules: - apiGroups: - \"coordination.k8s.io\" resources: - leases verbs: - get - create - update - apiGroups: - \"\" resources: - services/status verbs: - update","title":"RBAC permissions"},{"location":"load-balancer-allocator/#environment-variables","text":"The controller uses the environment variable POD_NAME as the identify for the lease used for leader election. Using the kubernetes downward api to set POD_NAME to the pod name the lease identify will match the current leader. --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-system spec: ... template: metadata: .... spec: ... env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name ... The environment variable POD_NAMESPACE can also be specified to set the namespace used for the lease. By default the namespace is looked up from within the pod using /var/run/secrets/kubernetes.io/serviceaccount/namespace .","title":"Environment variables"},{"location":"load-balancer-allocator/#running-outside-kubernetes","text":"When running the controller outside a pod, both POD_NAME and POD_NAMESPACE must set for the controller to work. POD_NAME should be unique per instance, so using for example the hostname of the machine might be a good idea. POD_NAMESPACE must be the same across all instances running in the same cluster.","title":"Running outside kubernetes"},{"location":"load-balancer-allocator/#notes","text":"It's not possible to specify the addresses for the load balancer services. A externalIP service can be used instead.","title":"Notes"},{"location":"metrics/","text":"Metrics Scraping kube-router metrics with Prometheus The scope of this document is to describe how to setup the annotations needed for Prometheus to use Kubernetes SD to discover & scape kube-router pods . For help with installing Prometheus please see their docs Metrics options: --metrics-path string Path to serve Prometheus metrics on ( default: /metrics ) --metrics-port uint16 <0-65535> Prometheus metrics port to use ( default: 0, disabled ) To enable kube-router metrics, start kube-router with --metrics-port and provide a port over 0 Metrics is generally exported at the same rate as the sync period for each service. The default values unless other specified is iptables-sync-period - `1 min`` ipvs-sync-period - `1 min`` routes-sync-period - `1 min`` By enabling Kubernetes SD in Prometheus configuration & adding required annotations Prometheus can automaticly discover & scrape kube-router metrics Version notes kube-router v0.2.4 received a metrics overhaul where some metrics were changed into histograms, additional metrics were also added. Please make sure you are using the latest dashboard version with versions => v0.2.4 kube-router 0.1.0-rc2 and upwards supports the runtime configuration for controlling where to expose the metrics. If you are using a older version, metrics path & port is locked to /metrics & 8080 Available metrics If metrics is enabled only services that are running have their metrics exposed The following metrics is exposed by kube-router prefixed by kube_router_ run-router = true controller_bgp_peers Number of BGP peers of the instance controller_bgp_advertisements_received Total number of BGP advertisements received since kube-router started controller_bgp_advertisements_sent Total number of BGP advertisements sent since kube-router started controller_bgp_internal_peers_sync_time Time it took for the BGP internal peer sync loop to complete controller_routes_sync_time Time it took for controller to sync routes run-firewall=true controller_iptables_sync_time Time it took for the iptables sync loop to complete controller_policy_chains_sync_time Time it took for controller to sync policy chains run-service-proxy = true controller_ipvs_services_sync_time Time it took for the ipvs sync loop to complete controller_ipvs_services The number of ipvs services in the instance controller_ipvs_metrics_export_time The time it took to run the metrics export for IPVS services service_total_connections Total connections made to the service since creation service_packets_in Total n/o packets received by service service_packets_out Total n/o packets sent by service service_bytes_in Total bytes received by the service service_bytes_out Total bytes sent by the service service_pps_in Incoming packets per second service_pps_out Outgoing packets per second service_cps Connections per second service_bps_in Incoming bytes per second service_bps_out Outgoing bytes per second To get a grouped list of CPS for each service a Prometheus query could look like this e.g: sum(kube_router_service_cps) by (svc_namespace, service_name) Grafana Dashboard This repo contains a example Grafana dashboard utilizing all the above exposed metrics from kube-router.","title":"Metrics"},{"location":"metrics/#metrics","text":"","title":"Metrics"},{"location":"metrics/#scraping-kube-router-metrics-with-prometheus","text":"The scope of this document is to describe how to setup the annotations needed for Prometheus to use Kubernetes SD to discover & scape kube-router pods . For help with installing Prometheus please see their docs Metrics options: --metrics-path string Path to serve Prometheus metrics on ( default: /metrics ) --metrics-port uint16 <0-65535> Prometheus metrics port to use ( default: 0, disabled ) To enable kube-router metrics, start kube-router with --metrics-port and provide a port over 0 Metrics is generally exported at the same rate as the sync period for each service. The default values unless other specified is iptables-sync-period - `1 min`` ipvs-sync-period - `1 min`` routes-sync-period - `1 min`` By enabling Kubernetes SD in Prometheus configuration & adding required annotations Prometheus can automaticly discover & scrape kube-router metrics","title":"Scraping kube-router metrics with Prometheus"},{"location":"metrics/#version-notes","text":"kube-router v0.2.4 received a metrics overhaul where some metrics were changed into histograms, additional metrics were also added. Please make sure you are using the latest dashboard version with versions => v0.2.4 kube-router 0.1.0-rc2 and upwards supports the runtime configuration for controlling where to expose the metrics. If you are using a older version, metrics path & port is locked to /metrics & 8080","title":"Version notes"},{"location":"metrics/#available-metrics","text":"If metrics is enabled only services that are running have their metrics exposed The following metrics is exposed by kube-router prefixed by kube_router_","title":"Available metrics"},{"location":"metrics/#run-router-true","text":"controller_bgp_peers Number of BGP peers of the instance controller_bgp_advertisements_received Total number of BGP advertisements received since kube-router started controller_bgp_advertisements_sent Total number of BGP advertisements sent since kube-router started controller_bgp_internal_peers_sync_time Time it took for the BGP internal peer sync loop to complete controller_routes_sync_time Time it took for controller to sync routes","title":"run-router = true"},{"location":"metrics/#run-firewalltrue","text":"controller_iptables_sync_time Time it took for the iptables sync loop to complete controller_policy_chains_sync_time Time it took for controller to sync policy chains","title":"run-firewall=true"},{"location":"metrics/#run-service-proxy-true","text":"controller_ipvs_services_sync_time Time it took for the ipvs sync loop to complete controller_ipvs_services The number of ipvs services in the instance controller_ipvs_metrics_export_time The time it took to run the metrics export for IPVS services service_total_connections Total connections made to the service since creation service_packets_in Total n/o packets received by service service_packets_out Total n/o packets sent by service service_bytes_in Total bytes received by the service service_bytes_out Total bytes sent by the service service_pps_in Incoming packets per second service_pps_out Outgoing packets per second service_cps Connections per second service_bps_in Incoming bytes per second service_bps_out Outgoing bytes per second To get a grouped list of CPS for each service a Prometheus query could look like this e.g: sum(kube_router_service_cps) by (svc_namespace, service_name)","title":"run-service-proxy = true"},{"location":"metrics/#grafana-dashboard","text":"This repo contains a example Grafana dashboard utilizing all the above exposed metrics from kube-router.","title":"Grafana Dashboard"},{"location":"observability/","text":"Observability Observing kube-router with Metrics See metrics documentation for more information Observing dropped traffic due to network policy enforcements Traffic that gets rejected due to network policy enforcements gets logged by kube-route using iptables NFLOG target under the group 100. Simplest way to observe the dropped packets by kube-router is by running tcpdump on nflog:100 interface for e.g. tcpdump -i nflog:100 -n . You can also configure ulogd to monitor dropped packets in desired output format. Please see the official ulogd documentation for an example configuration to setup a stack to log packets.","title":"Observability"},{"location":"observability/#observability","text":"","title":"Observability"},{"location":"observability/#observing-kube-router-with-metrics","text":"See metrics documentation for more information","title":"Observing kube-router with Metrics"},{"location":"observability/#observing-dropped-traffic-due-to-network-policy-enforcements","text":"Traffic that gets rejected due to network policy enforcements gets logged by kube-route using iptables NFLOG target under the group 100. Simplest way to observe the dropped packets by kube-router is by running tcpdump on nflog:100 interface for e.g. tcpdump -i nflog:100 -n . You can also configure ulogd to monitor dropped packets in desired output format. Please see the official ulogd documentation for an example configuration to setup a stack to log packets.","title":"Observing dropped traffic due to network policy enforcements"},{"location":"pod-toolbox/","text":"Pod Toolbox When kube-router is ran as a Pod within your Kubernetes cluster, it also ships with a number of tools automatically configured for your cluster. These can be used to troubleshoot issues and learn more about how cluster networking is performed. Logging In Here's a quick way to get going on a random node in your cluster: KR_POD=$(basename $(kubectl -n kube-system get pods -l k8s-app=kube-router --output name|head -n1)) kubectl -n kube-system exec -it ${KR_POD} bash Use kubectl -n kube-system get pods -l k8s-app=kube-router -o wide to see what nodes are running which pods. This will help if you want to investigate a particular node. Tools And Usage Once logged in you will see some help on using the tools in the container. For example: Welcome to kube-router on \"node1.zbrbdl\"! For debugging, the following tools are available: - ipvsadm | Gather info about Virtual Services and Real Servers via IPVS. | Examples: | ## Show all options | ipvsadm --help | ## List Services and Endpoints handled by IPVS | ipvsadm -ln | ## Show traffic rate information | ipvsadm -ln --rate | ## Show cumulative traffic | ipvsadm -ln --stats - gobgp | Get BGP related information from your nodes. | | Tab-completion is ready to use, just type \"gobgp <TAB>\" | to see the subcommands available. | | By default gobgp will query the Node this Pod is running | on, i.e. \"node1.zbrbdl\". To query a different node use | \"gobgp --host node02.mydomain\" as an example. | | For more examples see: https://github.com/osrg/gobgp/blob/master/docs/sources/cli-command-syntax.md Here's a quick look at what's happening on this Node --- BGP Server Configuration --- AS: 64512 Router-ID: 10.10.3.2 Listening Port: 179, Addresses: 0.0.0.0, :: --- BGP Neighbors --- Peer AS Up/Down State |#Received Accepted 64512 2d 01:05:07 Establ | 1 1 --- BGP Route Info --- Network Next Hop AS_PATH Age Attrs *> 10.2.0.0/24 10.10.3.3 4000 400000 300000 40001 2d 01:05:20 [{Origin: i} {LocalPref: 100}] *> 10.2.1.0/24 10.10.3.2 4000 400000 300000 40001 00:00:36 [{Origin: i}] --- IPVS Services --- IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.3.0.1:443 rr persistent 10800 mask 0.0.0.0 -> 10.10.3.2:443 Masq 1 0 0 TCP 10.3.0.10:53 rr -> 10.2.0.2:53 Masq 1 0 0 TCP 10.3.0.15:2379 rr -> 10.10.3.3:2379 Masq 1 45 0 TCP 10.3.0.155:2379 rr -> 10.10.3.3:2379 Masq 1 0 0 UDP 10.3.0.10:53 rr -> 10.2.0.2:53 Masq 1 0 0","title":"Pod tool box"},{"location":"pod-toolbox/#pod-toolbox","text":"When kube-router is ran as a Pod within your Kubernetes cluster, it also ships with a number of tools automatically configured for your cluster. These can be used to troubleshoot issues and learn more about how cluster networking is performed.","title":"Pod Toolbox"},{"location":"pod-toolbox/#logging-in","text":"Here's a quick way to get going on a random node in your cluster: KR_POD=$(basename $(kubectl -n kube-system get pods -l k8s-app=kube-router --output name|head -n1)) kubectl -n kube-system exec -it ${KR_POD} bash Use kubectl -n kube-system get pods -l k8s-app=kube-router -o wide to see what nodes are running which pods. This will help if you want to investigate a particular node.","title":"Logging In"},{"location":"pod-toolbox/#tools-and-usage","text":"Once logged in you will see some help on using the tools in the container. For example: Welcome to kube-router on \"node1.zbrbdl\"! For debugging, the following tools are available: - ipvsadm | Gather info about Virtual Services and Real Servers via IPVS. | Examples: | ## Show all options | ipvsadm --help | ## List Services and Endpoints handled by IPVS | ipvsadm -ln | ## Show traffic rate information | ipvsadm -ln --rate | ## Show cumulative traffic | ipvsadm -ln --stats - gobgp | Get BGP related information from your nodes. | | Tab-completion is ready to use, just type \"gobgp <TAB>\" | to see the subcommands available. | | By default gobgp will query the Node this Pod is running | on, i.e. \"node1.zbrbdl\". To query a different node use | \"gobgp --host node02.mydomain\" as an example. | | For more examples see: https://github.com/osrg/gobgp/blob/master/docs/sources/cli-command-syntax.md Here's a quick look at what's happening on this Node --- BGP Server Configuration --- AS: 64512 Router-ID: 10.10.3.2 Listening Port: 179, Addresses: 0.0.0.0, :: --- BGP Neighbors --- Peer AS Up/Down State |#Received Accepted 64512 2d 01:05:07 Establ | 1 1 --- BGP Route Info --- Network Next Hop AS_PATH Age Attrs *> 10.2.0.0/24 10.10.3.3 4000 400000 300000 40001 2d 01:05:20 [{Origin: i} {LocalPref: 100}] *> 10.2.1.0/24 10.10.3.2 4000 400000 300000 40001 00:00:36 [{Origin: i}] --- IPVS Services --- IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.3.0.1:443 rr persistent 10800 mask 0.0.0.0 -> 10.10.3.2:443 Masq 1 0 0 TCP 10.3.0.10:53 rr -> 10.2.0.2:53 Masq 1 0 0 TCP 10.3.0.15:2379 rr -> 10.10.3.3:2379 Masq 1 45 0 TCP 10.3.0.155:2379 rr -> 10.10.3.3:2379 Masq 1 0 0 UDP 10.3.0.10:53 rr -> 10.2.0.2:53 Masq 1 0 0","title":"Tools And Usage"},{"location":"see-it-in-action/","text":"See Kube-router in action Network Services Controller Network services controller is responsible for reading the services and endpoints information from Kubernetes API server and configure IPVS on each cluster node accordingly. Please read our blog for design details and pros and cons compared to iptables based Kube-proxy Demo of Kube-router's IPVS based Kubernetes network service proxy Features: round robin load balancing client IP based session persistence source IP is preserved if service controller is used in conjuction with network routes controller (kube-router with --run-router flag) option to explicitly masquerade (SNAT) with --masquerade-all flag Network Policy Controller Network policy controller is responsible for reading the namespace, network policy and pods information from Kubernetes API server and configure iptables accordingly to provide ingress filter to the pods. Kube-router supports the networking.k8s.io/NetworkPolicy API or network policy V1/GA semantics and also network policy beta semantics. Please read our blog for design details of Network Policy controller Demo of Kube-router's iptables based implementaton of network policies Network Routes Controller Network routes controller is responsible for reading pod CIDR allocated by controller manager to the node, and advertises the routes to the rest of the nodes in the cluster (BGP peers). Use of BGP is transparent to user for basic pod-to-pod networking. However BGP can be leveraged to other use cases like advertising the cluster ip, routable pod ip etc. Only in such use-cases understanding of BGP and configuration is required. Please see below demo how kube-router advertises cluster IP and pod cidrs to external BGP router","title":"See it in Action"},{"location":"see-it-in-action/#see-kube-router-in-action","text":"","title":"See Kube-router in action"},{"location":"see-it-in-action/#network-services-controller","text":"Network services controller is responsible for reading the services and endpoints information from Kubernetes API server and configure IPVS on each cluster node accordingly. Please read our blog for design details and pros and cons compared to iptables based Kube-proxy Demo of Kube-router's IPVS based Kubernetes network service proxy Features: round robin load balancing client IP based session persistence source IP is preserved if service controller is used in conjuction with network routes controller (kube-router with --run-router flag) option to explicitly masquerade (SNAT) with --masquerade-all flag","title":"Network Services Controller"},{"location":"see-it-in-action/#network-policy-controller","text":"Network policy controller is responsible for reading the namespace, network policy and pods information from Kubernetes API server and configure iptables accordingly to provide ingress filter to the pods. Kube-router supports the networking.k8s.io/NetworkPolicy API or network policy V1/GA semantics and also network policy beta semantics. Please read our blog for design details of Network Policy controller Demo of Kube-router's iptables based implementaton of network policies","title":"Network Policy Controller"},{"location":"see-it-in-action/#network-routes-controller","text":"Network routes controller is responsible for reading pod CIDR allocated by controller manager to the node, and advertises the routes to the rest of the nodes in the cluster (BGP peers). Use of BGP is transparent to user for basic pod-to-pod networking. However BGP can be leveraged to other use cases like advertising the cluster ip, routable pod ip etc. Only in such use-cases understanding of BGP and configuration is required. Please see below demo how kube-router advertises cluster IP and pod cidrs to external BGP router","title":"Network Routes Controller"},{"location":"troubleshoot/","text":"","title":"Troubleshoot"},{"location":"tunnels/","text":"Tunnels in kube-router There are several situations in which kube-router will use tunnels in order to perform certain forms of overlay / underlay routing within the cluster. To accomplish this, kube-router makes use of IPIP overlay tunnels that are built into the Linux kernel and instrumented with iproute2. Scenarios for Tunnelling By default, kube-router enables the option --enable-overlay which will perform overlay networking based upon the --overlay-type setting (by default set to subnet ). So out of the box, kube-router will create a tunnel for pod-to-pod traffic any time it comes across a kube-router enabled node that is not within the subnet of it's primary interface. Additionally, if --overlay-type is set to full kube-router will create an tunnel for all pod-to-pod traffic and attempt to transit any pod traffic in the cluster via an IPIP overlay network between nodes. Finally, kube-router also uses tunnels for DSR ( Direct Server Return ). In this case, the inbound traffic is encapsulated in an IPIP packet by IPVS after it reaches the node and before it is set to the pod for processing. This allows the return IP address of the sender to be preserved at the pod level so that it can be sent directly back to the requestor (rather than being routed in a synchronous fashion). Encapsulation Types IPIP (IP in IP) - This is the default method of encapsulation that kube-router uses FoU (Foo over UDP) - This is an optional type of IPIP encapsulation that kube-router uses if the user enables it FoU Details Specifically, kube-router uses GUE ( Generic UDP Encapsulation ) in order to support both IPv4 and IPv6 FoU tunnels. This option can be enabled via the kube-router parameter --overlay-encap=fou . Optionally, the user can also specify a desired port for this traffic via the --overlay-encap-port parameter (by default set to 5555 ). IPIP with Azure Unfortunately, Azure doesn't allow IPIP encapsulation on their network. So users that want to use an overlay network will need to enable fou support in order to deploy kube-router in an Azure environment. Changing Between Tunnel Types in a Live Cluster While it is possible to change a running cluster between ipip and fou type tunnels, administrators should beware that during the rollout it will cause pod-to-pod traffic to be dropped between nodes. Since, in almost all rollout scenarios, kube-router would be rolled out gracefully one pod or host to the next, during this rollout there will be mismatches between the encapsulation support between the two nodes as invariably one node will have an upgraded kube-router and another node may have a previous deployment. When this happens, they will have conflicting encapsulation setup on their tunnels and traffic will not be able to be sent between the two nodes until they are using a consistent encapsulation protocal between them. Once all nodes have upgraded with the destination configuration, pod-to-pod traffic patterns should return to normal.","title":"Tunneling"},{"location":"tunnels/#tunnels-in-kube-router","text":"There are several situations in which kube-router will use tunnels in order to perform certain forms of overlay / underlay routing within the cluster. To accomplish this, kube-router makes use of IPIP overlay tunnels that are built into the Linux kernel and instrumented with iproute2.","title":"Tunnels in kube-router"},{"location":"tunnels/#scenarios-for-tunnelling","text":"By default, kube-router enables the option --enable-overlay which will perform overlay networking based upon the --overlay-type setting (by default set to subnet ). So out of the box, kube-router will create a tunnel for pod-to-pod traffic any time it comes across a kube-router enabled node that is not within the subnet of it's primary interface. Additionally, if --overlay-type is set to full kube-router will create an tunnel for all pod-to-pod traffic and attempt to transit any pod traffic in the cluster via an IPIP overlay network between nodes. Finally, kube-router also uses tunnels for DSR ( Direct Server Return ). In this case, the inbound traffic is encapsulated in an IPIP packet by IPVS after it reaches the node and before it is set to the pod for processing. This allows the return IP address of the sender to be preserved at the pod level so that it can be sent directly back to the requestor (rather than being routed in a synchronous fashion).","title":"Scenarios for Tunnelling"},{"location":"tunnels/#encapsulation-types","text":"IPIP (IP in IP) - This is the default method of encapsulation that kube-router uses FoU (Foo over UDP) - This is an optional type of IPIP encapsulation that kube-router uses if the user enables it","title":"Encapsulation Types"},{"location":"tunnels/#fou-details","text":"Specifically, kube-router uses GUE ( Generic UDP Encapsulation ) in order to support both IPv4 and IPv6 FoU tunnels. This option can be enabled via the kube-router parameter --overlay-encap=fou . Optionally, the user can also specify a desired port for this traffic via the --overlay-encap-port parameter (by default set to 5555 ).","title":"FoU Details"},{"location":"tunnels/#ipip-with-azure","text":"Unfortunately, Azure doesn't allow IPIP encapsulation on their network. So users that want to use an overlay network will need to enable fou support in order to deploy kube-router in an Azure environment.","title":"IPIP with Azure"},{"location":"tunnels/#changing-between-tunnel-types-in-a-live-cluster","text":"While it is possible to change a running cluster between ipip and fou type tunnels, administrators should beware that during the rollout it will cause pod-to-pod traffic to be dropped between nodes. Since, in almost all rollout scenarios, kube-router would be rolled out gracefully one pod or host to the next, during this rollout there will be mismatches between the encapsulation support between the two nodes as invariably one node will have an upgraded kube-router and another node may have a previous deployment. When this happens, they will have conflicting encapsulation setup on their tunnels and traffic will not be able to be sent between the two nodes until they are using a consistent encapsulation protocal between them. Once all nodes have upgraded with the destination configuration, pod-to-pod traffic patterns should return to normal.","title":"Changing Between Tunnel Types in a Live Cluster"},{"location":"upgrading/","text":"Upgrading kube-router Breaking Changes We follow semantic versioning and try to the best of our abilities to maintain a stable interface between patch versions. For example, v0.1.1 -> v0.1.2 should be a perfectly safe upgrade path, without data service interruption. However, major ( vX.0.0 ) and minor ( v0.Y.0 ) version upgrades may contain breaking changes, which will be detailed here and in the release notes. First check if you are upgrading across one of the breaking change versions . If so, read the relevant section(s) first before proceeding with the general guidelines below. General Guidelines Image Pull Policy Here we will assume that you have the following in your kube-router DaemonSet: imagePullPolicy: Always If that's not the case, you will need to manually pull the desired image version on each of your nodes with a command like: docker pull cloudnativelabs/kube-router:VERSION Without Rolling Updates This is the default situation with our DaemonSet manifests. We will soon be switching these manifests to use Rolling Updates though. The following example(s) show an upgrade from v0.0.15 to v0.0.16 . First we will modify the kube-router DaemonSet resource's image field: kubectl -n kube-system set image ds/kube-router kube-router=cloudnativelabs/kube-router:v0.0.16 This does not actually trigger any version changes yet. It is recommended that you upgrade only one node and perform any tests you see fit to ensure nothing goes wrong. For example, we'll test upgrading kube-router on worker-01: TEST_NODE=\"worker-01\" TEST_POD=\"$(kubectl -n kube-system get pods -o wide|grep -E \"^kube-router.*${TEST_NODE}\"|awk '{ print $1 }')\" kubectl -n kube-system delete pod \"${TEST_POD}\" You can watch to make sure the new kube-router pod comes up and stays running with: kubectl -n kube-system get pods -o wide -w Check the logs with: TEST_NODE=\"worker-01\" TEST_POD=\"$(kubectl -n kube-system get pods -o wide|grep -E \"^kube-router.*${TEST_NODE}\"|awk '{ print $1 }')\" kubectl -n kube-system logs \"${TEST_POD}\" If it all looks good, go ahead and upgrade kube-router on all nodes: kubectl -n kube-system delete pods -l k8s-app=kube-router With Rolling Updates After updating a DaemonSet template, old DaemonSet pods will be killed, and new DaemonSet pods will be created automatically, in a controlled fashion If your global BGP peers supports gracefull restarts and has it enabled, rolling updates can be used to upgrade your kube-router DaemonSet without network downtime. To enable gracefull BGP restart kube-router must be started with --bgp-graceful-restart To enable rolling updates on your kube-router DaemonSet modify it and add a updateStrategy updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxUnavailable controls the maximum number of pods to simultaneously upgrade Starting from the top of the DaemonSet, it should look like this after you are done editing apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-system spec: updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 ... Breaking Change Version History This section covers version specific upgrade instructions. v0.0.X alpha versions While kube-router is in its alpha stage changes can be expected to be rapid. Therefor we cannot guarantee that a new alpha release will not break previous expected behavior. v0.0.17 (aka v0.1.0-rc1) This version brings changes to hairpin and BGP peering CLI/annotation configuration flags/keys. CLI flag changes: OLD: --peer-router -> NEW: --peer-router-ips OLD: --peer-asn -> NEW: --peer-router-asns CLI flag additions: NEW: --peer-router-passwords Annotation key changes: OLD: kube-router.io/hairpin-mode= -> NEW: kube-router.io/service.hairpin= OLD: net.kuberouter.nodeasn= -> NEW: kube-router.io/node.asn= OLD: net.kuberouter.node.bgppeer.address= -> NEW: kube-router.io/peer.ips OLD: net.kuberouter.node.bgppeer.asn -> NEW: kube-router.io/peer.asns Annotation key additions: NEW: kube-router.io/peer.passwords v0.0.17 Upgrade Procedure For CLI flag changes, all that is required is to change the flag names you use above to their new names at the same time that you change the image version. kubectl -n kube-system edit ds kube-router For Annotations, the recommended approach is to copy all the values of your current annotations into new annotations with the updated keys. You can get a quick look at all your service and node annotations with these commands: kubectl describe services --all-namespaces |grep -E '^(Name:|Annotations:)' kubectl describe nodes |grep -E '^(Name:|Annotations:)' For example if you have a service annotation to enable Hairpin mode like: Name: hairpin-service Annotations: kube-router.io/hairpin-mode= You will then want to make a new annotation with the new key: kubectl annotate service hairpin-service \"kube-router.io/service.hairpin=\" Once all new annotations are created, proceed with the General Guidelines . After the upgrades tested and complete, you can delete the old annotations. kubectl annotate service hairpin-service \"kube-router.io/hairpin-mode-\"","title":"Upgrading"},{"location":"upgrading/#upgrading-kube-router","text":"","title":"Upgrading kube-router"},{"location":"upgrading/#breaking-changes","text":"We follow semantic versioning and try to the best of our abilities to maintain a stable interface between patch versions. For example, v0.1.1 -> v0.1.2 should be a perfectly safe upgrade path, without data service interruption. However, major ( vX.0.0 ) and minor ( v0.Y.0 ) version upgrades may contain breaking changes, which will be detailed here and in the release notes. First check if you are upgrading across one of the breaking change versions . If so, read the relevant section(s) first before proceeding with the general guidelines below.","title":"Breaking Changes"},{"location":"upgrading/#general-guidelines","text":"","title":"General Guidelines"},{"location":"upgrading/#image-pull-policy","text":"Here we will assume that you have the following in your kube-router DaemonSet: imagePullPolicy: Always If that's not the case, you will need to manually pull the desired image version on each of your nodes with a command like: docker pull cloudnativelabs/kube-router:VERSION","title":"Image Pull Policy"},{"location":"upgrading/#without-rolling-updates","text":"This is the default situation with our DaemonSet manifests. We will soon be switching these manifests to use Rolling Updates though. The following example(s) show an upgrade from v0.0.15 to v0.0.16 . First we will modify the kube-router DaemonSet resource's image field: kubectl -n kube-system set image ds/kube-router kube-router=cloudnativelabs/kube-router:v0.0.16 This does not actually trigger any version changes yet. It is recommended that you upgrade only one node and perform any tests you see fit to ensure nothing goes wrong. For example, we'll test upgrading kube-router on worker-01: TEST_NODE=\"worker-01\" TEST_POD=\"$(kubectl -n kube-system get pods -o wide|grep -E \"^kube-router.*${TEST_NODE}\"|awk '{ print $1 }')\" kubectl -n kube-system delete pod \"${TEST_POD}\" You can watch to make sure the new kube-router pod comes up and stays running with: kubectl -n kube-system get pods -o wide -w Check the logs with: TEST_NODE=\"worker-01\" TEST_POD=\"$(kubectl -n kube-system get pods -o wide|grep -E \"^kube-router.*${TEST_NODE}\"|awk '{ print $1 }')\" kubectl -n kube-system logs \"${TEST_POD}\" If it all looks good, go ahead and upgrade kube-router on all nodes: kubectl -n kube-system delete pods -l k8s-app=kube-router","title":"Without Rolling Updates"},{"location":"upgrading/#with-rolling-updates","text":"After updating a DaemonSet template, old DaemonSet pods will be killed, and new DaemonSet pods will be created automatically, in a controlled fashion If your global BGP peers supports gracefull restarts and has it enabled, rolling updates can be used to upgrade your kube-router DaemonSet without network downtime. To enable gracefull BGP restart kube-router must be started with --bgp-graceful-restart To enable rolling updates on your kube-router DaemonSet modify it and add a updateStrategy updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxUnavailable controls the maximum number of pods to simultaneously upgrade Starting from the top of the DaemonSet, it should look like this after you are done editing apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: k8s-app: kube-router tier: node name: kube-router namespace: kube-system spec: updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 ...","title":"With Rolling Updates"},{"location":"upgrading/#breaking-change-version-history","text":"This section covers version specific upgrade instructions.","title":"Breaking Change Version History"},{"location":"upgrading/#v00x-alpha-versions","text":"While kube-router is in its alpha stage changes can be expected to be rapid. Therefor we cannot guarantee that a new alpha release will not break previous expected behavior.","title":"v0.0.X alpha versions"},{"location":"upgrading/#v0017-aka-v010-rc1","text":"This version brings changes to hairpin and BGP peering CLI/annotation configuration flags/keys. CLI flag changes: OLD: --peer-router -> NEW: --peer-router-ips OLD: --peer-asn -> NEW: --peer-router-asns CLI flag additions: NEW: --peer-router-passwords Annotation key changes: OLD: kube-router.io/hairpin-mode= -> NEW: kube-router.io/service.hairpin= OLD: net.kuberouter.nodeasn= -> NEW: kube-router.io/node.asn= OLD: net.kuberouter.node.bgppeer.address= -> NEW: kube-router.io/peer.ips OLD: net.kuberouter.node.bgppeer.asn -> NEW: kube-router.io/peer.asns Annotation key additions: NEW: kube-router.io/peer.passwords","title":"v0.0.17 (aka v0.1.0-rc1)"},{"location":"upgrading/#v0017-upgrade-procedure","text":"For CLI flag changes, all that is required is to change the flag names you use above to their new names at the same time that you change the image version. kubectl -n kube-system edit ds kube-router For Annotations, the recommended approach is to copy all the values of your current annotations into new annotations with the updated keys. You can get a quick look at all your service and node annotations with these commands: kubectl describe services --all-namespaces |grep -E '^(Name:|Annotations:)' kubectl describe nodes |grep -E '^(Name:|Annotations:)' For example if you have a service annotation to enable Hairpin mode like: Name: hairpin-service Annotations: kube-router.io/hairpin-mode= You will then want to make a new annotation with the new key: kubectl annotate service hairpin-service \"kube-router.io/service.hairpin=\" Once all new annotations are created, proceed with the General Guidelines . After the upgrades tested and complete, you can delete the old annotations. kubectl annotate service hairpin-service \"kube-router.io/hairpin-mode-\"","title":"v0.0.17 Upgrade Procedure"},{"location":"user-guide/","text":"User Guide Try Kube-router with cluster installers The best way to get started is to deploy Kubernetes with Kube-router is with a cluster installer. kops Please see the steps to deploy Kubernetes cluster with Kube-router using Kops kubeadm Please see the steps to deploy Kubernetes cluster with Kube-router using Kubeadm k0sproject k0s by default uses kube-router as a CNI option. Please see the steps to deploy Kubernetes cluster with Kube-router using k0s k3sproject k3s by default uses kube-router's network policy controller implementation for its NetworkPolicy enforcement. generic Please see the steps to deploy kube-router on manually installed clusters Amazon specific notes When running in an AWS environment that requires an explicit proxy you need to inject the proxy server as a environment variable in your kube-router deployment Example: env: - name: HTTP_PROXY value: \"http://proxy.example.com:80\" Azure specific notes Azure does not support IPIP packet encapsulation which is the default packet encapsulation that kube-router uses. If you need to use an overlay network in an Azure environment with kube-router, please ensure that you set --overlay-encap=fou . See kube-router Tunnel Documentation for more information. deployment Depending on what functionality of kube-router you want to use, multiple deployment options are possible. You can use the flags --run-firewall , --run-router , --run-service-proxy , --run-loadbalancer to selectively enable only required functionality of kube-router. Also you can choose to run kube-router as agent running on each cluster node. Alternativley you can run kube-router as pod on each node through daemonset. command line options Usage of kube-router: --advertise-cluster-ip Add Cluster IP of the service to the RIB so that it gets advertises to the BGP peers. --advertise-external-ip Add External IP of service to the RIB so that it gets advertised to the BGP peers. --advertise-loadbalancer-ip Add LoadbBalancer IP of service status as set by the LB provider to the RIB so that it gets advertised to the BGP peers. --advertise-pod-cidr Add Node's POD cidr to the RIB so that it gets advertised to the BGP peers. (default true) --auto-mtu Auto detect and set the largest possible MTU for kube-bridge and pod interfaces (also accounts for IPIP overlay network when enabled). (default true) --bgp-graceful-restart Enables the BGP Graceful Restart capability so that routes are preserved on unexpected restarts --bgp-graceful-restart-deferral-time duration BGP Graceful restart deferral time according to RFC4724 4.1, maximum 18h. (default 6m0s) --bgp-graceful-restart-time duration BGP Graceful restart time according to RFC4724 3, maximum 4095s. (default 1m30s) --bgp-holdtime duration This parameter is mainly used to modify the holdtime declared to BGP peer. When Kube-router goes down abnormally, the local saving time of BGP route will be affected. Holdtime must be in the range 3s to 18h12m16s. (default 1m30s) --bgp-port uint32 The port open for incoming BGP connections and to use for connecting with other BGP peers. (default 179) --cache-sync-timeout duration The timeout for cache synchronization (e.g. '5s', '1m'). Must be greater than 0. (default 1m0s) --cleanup-config Cleanup iptables rules, ipvs, ipset configuration and exit. --cluster-asn uint ASN number under which cluster nodes will run iBGP. --disable-source-dest-check Disable the source-dest-check attribute for AWS EC2 instances. When this option is false, it must be set some other way. (default true) --enable-cni Enable CNI plugin. Disable if you want to use kube-router features alongside another CNI plugin. (default true) --enable-ibgp Enables peering with nodes with the same ASN, if disabled will only peer with external BGP peers (default true) --enable-ipv4 Enables IPv4 support (default true) --enable-ipv6 Enables IPv6 support --enable-overlay When enable-overlay is set to true, IP-in-IP tunneling is used for pod-to-pod networking across nodes in different subnets. When set to false no tunneling is used and routing infrastructure is expected to route traffic for pod-to-pod networking across nodes in different subnets (default true) --enable-pod-egress SNAT traffic from Pods to destinations outside the cluster. (default true) --enable-pprof Enables pprof for debugging performance and memory leak issues. --excluded-cidrs strings Excluded CIDRs are used to exclude IPVS rules from deletion. --hairpin-mode Add iptables rules for every Service Endpoint to support hairpin traffic. --health-port uint16 Health check port, 0 = Disabled (default 20244) -h, --help Print usage information. --hostname-override string Overrides the NodeName of the node. Set this if kube-router is unable to determine your NodeName automatically. --injected-routes-sync-period duration The delay between route table synchronizations (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 1m0s) --iptables-sync-period duration The delay between iptables rule synchronizations (e.g. '5s', '1m'). Must be greater than 0. (default 5m0s) --ipvs-graceful-period duration The graceful period before removing destinations from IPVS services (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 30s) --ipvs-graceful-termination Enables the experimental IPVS graceful terminaton capability --ipvs-permit-all Enables rule to accept all incoming traffic to service VIP's on the node. (default true) --ipvs-sync-period duration The delay between ipvs config synchronizations (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 5m0s) --kubeconfig string Path to kubeconfig file with authorization information (the master location is set by the master flag). --loadbalancer-default-class Handle loadbalancer services without a class (default true) --loadbalancer-ip-range strings CIDR values from which loadbalancer services addresses are assigned (can be specified multiple times) --loadbalancer-sync-period duration The delay between checking for missed services (e.g. '5s', '1m'). Must be greater than 0. (default 1m0s) --masquerade-all SNAT all traffic to cluster IP/node port. --master string The address of the Kubernetes API server (overrides any value in kubeconfig). --metrics-addr string Prometheus metrics address to listen on, (Default: all interfaces) --metrics-path string Prometheus metrics path (default \"/metrics\") --metrics-port uint16 Prometheus metrics port, (Default 0, Disabled) --nodeport-bindon-all-ip For service of NodePort type create IPVS service that listens on all IP's of the node. --nodes-full-mesh Each node in the cluster will setup BGP peering with rest of the nodes. (default true) --overlay-encap string Valid encapsulation types are \"ipip\" or \"fou\" (if set to \"fou\", the udp port can be specified via \"overlay-encap-port\") (default \"ipip\") --overlay-encap-port uint16 Overlay tunnel encapsulation port (only used for \"fou\" encapsulation) (default 5555) --overlay-type string Possible values: subnet,full - When set to \"subnet\", the default, default \"--enable-overlay=true\" behavior is used. When set to \"full\", it changes \"--enable-overlay=true\" default behavior so that IP-in-IP tunneling is used for pod-to-pod networking across nodes regardless of the subnet the nodes are in. (default \"subnet\") --override-nexthop Override the next-hop in bgp routes sent to peers with the local ip. --peer-router-asns uints ASN numbers of the BGP peer to which cluster nodes will advertise cluster ip and node's pod cidr. (default []) --peer-router-ips ipSlice The ip address of the external router to which all nodes will peer and advertise the cluster ip and pod cidr's. (default []) --peer-router-multihop-ttl uint8 Enable eBGP multihop supports -- sets multihop-ttl. (Relevant only if ttl >= 2) --peer-router-passwords strings Password for authenticating against the BGP peer defined with \"--peer-router-ips\". --peer-router-passwords-file string Path to file containing password for authenticating against the BGP peer defined with \"--peer-router-ips\". --peer-router-passwords will be preferred if both are set. --peer-router-ports uints The remote port of the external BGP to which all nodes will peer. If not set, default BGP port (179) will be used. (default []) --router-id string BGP router-id. Must be specified in a ipv6 only cluster, \"generate\" can be specified to generate the router id. --routes-sync-period duration The delay between route updates and advertisements (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 5m0s) --run-firewall Enables Network Policy -- sets up iptables to provide ingress firewall for pods. (default true) --run-loadbalancer Enable loadbalancer address allocator --run-router Enables Pod Networking -- Advertises and learns the routes to Pods via iBGP. (default true) --run-service-proxy Enables Service Proxy -- sets up IPVS for Kubernetes Services. (default true) --runtime-endpoint string Path to CRI compatible container runtime socket (used for DSR mode). Currently known working with containerd. --service-cluster-ip-range strings CIDR values from which service cluster IPs are assigned (can be specified up to 2 times) (default [10.96.0.0/12]) --service-external-ip-range strings Specify external IP CIDRs that are used for inter-cluster communication (can be specified multiple times) --service-node-port-range string NodePort range specified with either a hyphen or colon (default \"30000-32767\") --service-tcp-timeout duration Specify TCP timeout for IPVS services in standard duration syntax (e.g. '5s', '1m'), default 0s preserves default system value (default: 0s) --service-tcpfin-timeout duration Specify TCP FIN timeout for IPVS services in standard duration syntax (e.g. '5s', '1m'), default 0s preserves default system value (default: 0s) --service-udp-timeout duration Specify UDP timeout for IPVS services in standard duration syntax (e.g. '5s', '1m'), default 0s preserves default system value (default: 0s) -v, --v string log level for V logs (default \"0\") -V, --version Print version information. requirements Kube-router need to access kubernetes API server to get information on pods, services, endpoints, network policies etc. The very minimum information it requires is the details on where to access the kubernetes API server. This information can be passed as: kube-router --master=http://192.168.1.99:8080/` or `kube-router --kubeconfig=<path to kubeconfig file> If you run kube-router as agent on the node, ipset package must be installed on each of the nodes (when run as daemonset, container image is prepackaged with ipset) If you choose to use kube-router for pod-to-pod network connectivity then Kubernetes controller manager need to be configured to allocate pod CIDRs by passing --allocate-node-cidrs=true flag and providing a cluster-cidr (i.e. by passing --cluster-cidr=10.1.0.0/16 for e.g.) If you choose to run kube-router as daemonset in Kubernetes version below v1.15, both kube-apiserver and kubelet must be run with --allow-privileged=true option. In later Kubernetes versions, only kube-apiserver must be run with --allow-privileged=true option and if PodSecurityPolicy admission controller is enabled, you should create PodSecurityPolicy, allowing privileged kube-router pods. Additionally, when run in daemonset mode, it is highly recommended that you keep netfilter related userspace host tooling like iptables , ipset , and ipvsadm in sync with the versions that are distributed by Alpine inside the kube-router container. This will help avoid conflicts that can potentially arise when both the host's userspace and kube-router's userspace tooling modifies netfilter kernel definitions. See: this kube-router issue for more information. If you choose to use kube-router for pod-to-pod network connecitvity then Kubernetes cluster must be configured to use CNI network plugins. On each node CNI conf file is expected to be present as /etc/cni/net.d/10-kuberouter.conf bridge CNI plugin and host-local for IPAM should be used. A sample conf file that can be downloaded as wget -O /etc/cni/net.d/10-kuberouter.conf https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/cni/10-kuberouter.conf` Additionally, the aforementioned bridge and host-local CNI plugins need to exist for the container runtime to reference if you have kube-router manage the pod-to-pod network. Additionally, if you use hostPort 's on any of your pods, you'll need to install the hostport plugin. As of kube-router v2.1.X, these plugins will be installed to /opt/cni/bin for you during the initContainer phase if kube-router finds them missing. Most container runtimes will know to look for your plugins there by default, however, you may have to configure them if you are having problems with your pods coming up. containerd configuration cri-o configuration cri-dockerd configuration running as daemonset This is quickest way to deploy kube-router in Kubernetes ( dont forget to ensure the requirements above ). Just run: kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kube-router-all-service-daemonset.yaml Above will run kube-router as pod on each node automatically. You can change the arguments in the daemonset definition as required to suit your needs. Some sample deployment configuration can be found in our daemonset examples with different arguments used to select a set of the services kube-router should run. running as agent You can choose to run kube-router as agent runnng on each node. For e.g if you just want kube-router to provide ingress firewall for the pods then you can start kube-router as: kube-router --master=http://192.168.1.99:8080/ --run-firewall=true --run-service-proxy=false --run-router=false cleanup configuration Please delete kube-router daemonset and then clean up all the configurations done (to ipvs, iptables, ipset, ip routes etc) by kube-router on the node by running below command. Docker docker run --privileged --net=host \\ --mount type=bind,source=/lib/modules,target=/lib/modules,readonly \\ --mount type=bind,source=/run/xtables.lock,target=/run/xtables.lock,bind-propagation=rshared \\ cloudnativelabs/kube-router /usr/local/bin/kube-router --cleanup-config containerd $ ctr image pull docker.io/cloudnativelabs/kube-router:latest $ ctr run --privileged -t --net-host \\ --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro \\ --mount type=bind,src=/run/xtables.lock,dst=/run/xtables.lock,options=rbind:rw \\ docker.io/cloudnativelabs/kube-router:latest kube-router-cleanup /usr/local/bin/kube-router --cleanup-config trying kube-router as alternative to kube-proxy If you have a kube-proxy in use, and want to try kube-router just for service proxy you can do kube-proxy --cleanup-iptables followed by kube-router --master=http://192.168.1.99:8080/ --run-service-proxy=true --run-firewall=false --run-router=false and if you want to move back to kube-proxy then clean up config done by kube-router by running kube-router --cleanup-config and run kube-proxy with the configuration you have. Advertising IPs kube-router can advertise Cluster, External and LoadBalancer IPs to BGP peers. It does this by: locally adding the advertised IPs to the nodes' kube-dummy-if network interface advertising the IPs to its BGP peers To set the default for all services use the --advertise-cluster-ip , --advertise-external-ip and --advertise-loadbalancer-ip flags. To selectively enable or disable this feature per-service use the kube-router.io/service.advertise.clusterip , kube-router.io/service.advertise.externalip and kube-router.io/service.advertise.loadbalancerip annotations. e.g.: $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.clusterip=true\" $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.externalip=true\" $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.loadbalancerip=true\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.clusterip=false\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.externalip=false\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.loadbalancerip=false\" By combining the flags with the per-service annotations you can choose either a opt-in or opt-out strategy for advertising IPs. Advertising LoadBalancer IPs works by inspecting the services status.loadBalancer.ingress IPs that are set by external LoadBalancers like for example MetalLb. This has been successfully tested together with MetalLB in ARP mode. Controlling Service Locality / Traffic Policies Service availability both externally and locally (within the cluster) can be controlled via the Kubernetes standard Traffic Policies and via the custom kube-router service annotation: kube-router.io/service.local: true . Refer to the previously linked upstream Kubernetes documentation for more information on spec.internalTrafficPolicy and spec.externalTrafficPolicy . In order to keep backwards compatibility the kube-router.io/service.local: true annotation effectively overrides spec.internalTrafficPolicy and spec.externalTrafficPolicy and forces kube-router to behave as if both were set to Local . Hairpin Mode Communication from a Pod that is behind a Service to its own ClusterIP:Port is not supported by default. However, it can be enabled per-service by adding the kube-router.io/service.hairpin= annotation, or for all Services in a cluster by passing the flag --hairpin-mode=true to kube-router. Additionally, the hairpin_mode sysctl option must be set to 1 for all veth interfaces on each node. This can be done by adding the \"hairpinMode\": true option to your CNI configuration and rebooting all cluster nodes if they are already running kubernetes. Hairpin traffic will be seen by the pod it originated from as coming from the Service ClusterIP if it is logging the source IP. Hairpin Mode Example 10-kuberouter.conf { \"name\":\"mynet\", \"type\":\"bridge\", \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"hairpinMode\":true, \"ipam\": { \"type\":\"host-local\" } } To enable hairpin traffic for Service my-service : kubectl annotate service my-service \"kube-router.io/service.hairpin=\" If you want to also hairpin externalIPs declared for Service my-service (note, you must also either enable global hairpin or service hairpin (see above ^^^) for this to have an effect): kubectl annotate service my-service \"kube-router.io/service.hairpin.externalips=\" SNATing Service Traffic By default, as traffic ingresses into the cluster, kube-router will source nat the traffic to ensure symmetric routing if it needs to proxy that traffic to ensure it gets to a node that has a service pod that is capable of servicing the traffic. This has a potential to cause issues when network policies are applied to that service since now the traffic will appear to be coming from a node in your cluster instead of the traffic originator. This is an issue that is common to all proxy's and all Kubernetes service proxies in general. You can read more information about this issue at: Source IP for Services In addition to the fix mentioned in the linked upstream documentation (using service.spec.externalTrafficPolicy ), kube-router also provides DSR , which by its nature preserves the source IP, to solve this problem. For more information see the section above. Load balancing Scheduling Algorithms Kube-router uses LVS for service proxy. LVS support rich set of scheduling alogirthms . You can annotate the service to choose one of the scheduling alogirthms. When a service is not annotated round-robin scheduler is selected by default #For least connection scheduling use: $ kubectl annotate service my-service \"kube-router.io/service.scheduler=lc\" #For round-robin scheduling use: $ kubectl annotate service my-service \"kube-router.io/service.scheduler=rr\" #For source hashing scheduling use: $ kubectl annotate service my-service \"kube-router.io/service.scheduler=sh\" #For destination hashing scheduling use: $ kubectl annotate service my-service \"kube-router.io/service.scheduler=dh\" HostPort support If you would like to use HostPort functionality below changes are required in the manifest. By default kube-router assumes CNI conf file to be /etc/cni/net.d/10-kuberouter.conf . Add an environment variable KUBE_ROUTER_CNI_CONF_FILE to kube-router manifest and set it to /etc/cni/net.d/10-kuberouter.conflist Modify kube-router-cfg ConfigMap with CNI config that supports portmap as additional plug-in { \"cniVersion\":\"0.3.0\", \"name\":\"mynet\", \"plugins\":[ { \"name\":\"kubernetes\", \"type\":\"bridge\", \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"ipam\":{ \"type\":\"host-local\" } }, { \"type\":\"portmap\", \"capabilities\":{ \"snat\":true, \"portMappings\":true } } ] } Update init container command to create /etc/cni/net.d/10-kuberouter.conflist file Restart the container runtime For an e.g manifest please look at manifest with necessary changes required for HostPort functionality. IPVS Graceful termination support As of 0.2.6 we support experimental graceful termination of IPVS destinations. When possible the pods's TerminationGracePeriodSeconds is used, if it cannot be retrived for some reason the fallback period is 30 seconds and can be adjusted with --ipvs-graceful-period cli-opt graceful termination works in such a way that when kube-router receives a delete endpoint notification for a service it's weight is adjusted to 0 before getting deleted after he termination grace period has passed or the Active & Inactive connections goes down to 0. MTU The maximum transmission unit (MTU) determines the largest packet size that can be transmitted through your network. MTU for the pod interfaces should be set appropriately to prevent fragmentation and packet drops thereby achieving maximum performance. If auto-mtu is set to true ( auto-mtu is set to true by default as of kube-router 1.1), kube-router will determine right MTU for both kube-bridge and pod interfaces. If you set auto-mtu to false kube-router will not attempt to configure MTU. However you can choose the right MTU and set in the cni-conf.json section of the 10-kuberouter.conflist in the kube-router daemonsets . For e.g. cni-conf.json: | { \"cniVersion\":\"0.3.0\", \"name\":\"mynet\", \"plugins\":[ { \"name\":\"kubernetes\", \"type\":\"bridge\", \"mtu\": 1400, \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"ipam\":{ \"type\":\"host-local\" } } ] } If you set MTU yourself via the CNI config, you'll also need to set MTU of kube-bridge manually to the right value to avoid packet fragmentation in case of existing nodes on which kube-bridge is already created. On node reboot or in case of new nodes joining the cluster both the pod's interface and kube-bridge will be setup with specified MTU value. BGP configuration Configuring BGP Peers Metrics Configure metrics gathering","title":"User Guide"},{"location":"user-guide/#user-guide","text":"","title":"User Guide"},{"location":"user-guide/#try-kube-router-with-cluster-installers","text":"The best way to get started is to deploy Kubernetes with Kube-router is with a cluster installer.","title":"Try Kube-router with cluster installers"},{"location":"user-guide/#kops","text":"Please see the steps to deploy Kubernetes cluster with Kube-router using Kops","title":"kops"},{"location":"user-guide/#kubeadm","text":"Please see the steps to deploy Kubernetes cluster with Kube-router using Kubeadm","title":"kubeadm"},{"location":"user-guide/#k0sproject","text":"k0s by default uses kube-router as a CNI option. Please see the steps to deploy Kubernetes cluster with Kube-router using k0s","title":"k0sproject"},{"location":"user-guide/#k3sproject","text":"k3s by default uses kube-router's network policy controller implementation for its NetworkPolicy enforcement.","title":"k3sproject"},{"location":"user-guide/#generic","text":"Please see the steps to deploy kube-router on manually installed clusters","title":"generic"},{"location":"user-guide/#amazon-specific-notes","text":"When running in an AWS environment that requires an explicit proxy you need to inject the proxy server as a environment variable in your kube-router deployment Example: env: - name: HTTP_PROXY value: \"http://proxy.example.com:80\"","title":"Amazon specific notes"},{"location":"user-guide/#azure-specific-notes","text":"Azure does not support IPIP packet encapsulation which is the default packet encapsulation that kube-router uses. If you need to use an overlay network in an Azure environment with kube-router, please ensure that you set --overlay-encap=fou . See kube-router Tunnel Documentation for more information.","title":"Azure specific notes"},{"location":"user-guide/#deployment","text":"Depending on what functionality of kube-router you want to use, multiple deployment options are possible. You can use the flags --run-firewall , --run-router , --run-service-proxy , --run-loadbalancer to selectively enable only required functionality of kube-router. Also you can choose to run kube-router as agent running on each cluster node. Alternativley you can run kube-router as pod on each node through daemonset.","title":"deployment"},{"location":"user-guide/#command-line-options","text":"Usage of kube-router: --advertise-cluster-ip Add Cluster IP of the service to the RIB so that it gets advertises to the BGP peers. --advertise-external-ip Add External IP of service to the RIB so that it gets advertised to the BGP peers. --advertise-loadbalancer-ip Add LoadbBalancer IP of service status as set by the LB provider to the RIB so that it gets advertised to the BGP peers. --advertise-pod-cidr Add Node's POD cidr to the RIB so that it gets advertised to the BGP peers. (default true) --auto-mtu Auto detect and set the largest possible MTU for kube-bridge and pod interfaces (also accounts for IPIP overlay network when enabled). (default true) --bgp-graceful-restart Enables the BGP Graceful Restart capability so that routes are preserved on unexpected restarts --bgp-graceful-restart-deferral-time duration BGP Graceful restart deferral time according to RFC4724 4.1, maximum 18h. (default 6m0s) --bgp-graceful-restart-time duration BGP Graceful restart time according to RFC4724 3, maximum 4095s. (default 1m30s) --bgp-holdtime duration This parameter is mainly used to modify the holdtime declared to BGP peer. When Kube-router goes down abnormally, the local saving time of BGP route will be affected. Holdtime must be in the range 3s to 18h12m16s. (default 1m30s) --bgp-port uint32 The port open for incoming BGP connections and to use for connecting with other BGP peers. (default 179) --cache-sync-timeout duration The timeout for cache synchronization (e.g. '5s', '1m'). Must be greater than 0. (default 1m0s) --cleanup-config Cleanup iptables rules, ipvs, ipset configuration and exit. --cluster-asn uint ASN number under which cluster nodes will run iBGP. --disable-source-dest-check Disable the source-dest-check attribute for AWS EC2 instances. When this option is false, it must be set some other way. (default true) --enable-cni Enable CNI plugin. Disable if you want to use kube-router features alongside another CNI plugin. (default true) --enable-ibgp Enables peering with nodes with the same ASN, if disabled will only peer with external BGP peers (default true) --enable-ipv4 Enables IPv4 support (default true) --enable-ipv6 Enables IPv6 support --enable-overlay When enable-overlay is set to true, IP-in-IP tunneling is used for pod-to-pod networking across nodes in different subnets. When set to false no tunneling is used and routing infrastructure is expected to route traffic for pod-to-pod networking across nodes in different subnets (default true) --enable-pod-egress SNAT traffic from Pods to destinations outside the cluster. (default true) --enable-pprof Enables pprof for debugging performance and memory leak issues. --excluded-cidrs strings Excluded CIDRs are used to exclude IPVS rules from deletion. --hairpin-mode Add iptables rules for every Service Endpoint to support hairpin traffic. --health-port uint16 Health check port, 0 = Disabled (default 20244) -h, --help Print usage information. --hostname-override string Overrides the NodeName of the node. Set this if kube-router is unable to determine your NodeName automatically. --injected-routes-sync-period duration The delay between route table synchronizations (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 1m0s) --iptables-sync-period duration The delay between iptables rule synchronizations (e.g. '5s', '1m'). Must be greater than 0. (default 5m0s) --ipvs-graceful-period duration The graceful period before removing destinations from IPVS services (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 30s) --ipvs-graceful-termination Enables the experimental IPVS graceful terminaton capability --ipvs-permit-all Enables rule to accept all incoming traffic to service VIP's on the node. (default true) --ipvs-sync-period duration The delay between ipvs config synchronizations (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 5m0s) --kubeconfig string Path to kubeconfig file with authorization information (the master location is set by the master flag). --loadbalancer-default-class Handle loadbalancer services without a class (default true) --loadbalancer-ip-range strings CIDR values from which loadbalancer services addresses are assigned (can be specified multiple times) --loadbalancer-sync-period duration The delay between checking for missed services (e.g. '5s', '1m'). Must be greater than 0. (default 1m0s) --masquerade-all SNAT all traffic to cluster IP/node port. --master string The address of the Kubernetes API server (overrides any value in kubeconfig). --metrics-addr string Prometheus metrics address to listen on, (Default: all interfaces) --metrics-path string Prometheus metrics path (default \"/metrics\") --metrics-port uint16 Prometheus metrics port, (Default 0, Disabled) --nodeport-bindon-all-ip For service of NodePort type create IPVS service that listens on all IP's of the node. --nodes-full-mesh Each node in the cluster will setup BGP peering with rest of the nodes. (default true) --overlay-encap string Valid encapsulation types are \"ipip\" or \"fou\" (if set to \"fou\", the udp port can be specified via \"overlay-encap-port\") (default \"ipip\") --overlay-encap-port uint16 Overlay tunnel encapsulation port (only used for \"fou\" encapsulation) (default 5555) --overlay-type string Possible values: subnet,full - When set to \"subnet\", the default, default \"--enable-overlay=true\" behavior is used. When set to \"full\", it changes \"--enable-overlay=true\" default behavior so that IP-in-IP tunneling is used for pod-to-pod networking across nodes regardless of the subnet the nodes are in. (default \"subnet\") --override-nexthop Override the next-hop in bgp routes sent to peers with the local ip. --peer-router-asns uints ASN numbers of the BGP peer to which cluster nodes will advertise cluster ip and node's pod cidr. (default []) --peer-router-ips ipSlice The ip address of the external router to which all nodes will peer and advertise the cluster ip and pod cidr's. (default []) --peer-router-multihop-ttl uint8 Enable eBGP multihop supports -- sets multihop-ttl. (Relevant only if ttl >= 2) --peer-router-passwords strings Password for authenticating against the BGP peer defined with \"--peer-router-ips\". --peer-router-passwords-file string Path to file containing password for authenticating against the BGP peer defined with \"--peer-router-ips\". --peer-router-passwords will be preferred if both are set. --peer-router-ports uints The remote port of the external BGP to which all nodes will peer. If not set, default BGP port (179) will be used. (default []) --router-id string BGP router-id. Must be specified in a ipv6 only cluster, \"generate\" can be specified to generate the router id. --routes-sync-period duration The delay between route updates and advertisements (e.g. '5s', '1m', '2h22m'). Must be greater than 0. (default 5m0s) --run-firewall Enables Network Policy -- sets up iptables to provide ingress firewall for pods. (default true) --run-loadbalancer Enable loadbalancer address allocator --run-router Enables Pod Networking -- Advertises and learns the routes to Pods via iBGP. (default true) --run-service-proxy Enables Service Proxy -- sets up IPVS for Kubernetes Services. (default true) --runtime-endpoint string Path to CRI compatible container runtime socket (used for DSR mode). Currently known working with containerd. --service-cluster-ip-range strings CIDR values from which service cluster IPs are assigned (can be specified up to 2 times) (default [10.96.0.0/12]) --service-external-ip-range strings Specify external IP CIDRs that are used for inter-cluster communication (can be specified multiple times) --service-node-port-range string NodePort range specified with either a hyphen or colon (default \"30000-32767\") --service-tcp-timeout duration Specify TCP timeout for IPVS services in standard duration syntax (e.g. '5s', '1m'), default 0s preserves default system value (default: 0s) --service-tcpfin-timeout duration Specify TCP FIN timeout for IPVS services in standard duration syntax (e.g. '5s', '1m'), default 0s preserves default system value (default: 0s) --service-udp-timeout duration Specify UDP timeout for IPVS services in standard duration syntax (e.g. '5s', '1m'), default 0s preserves default system value (default: 0s) -v, --v string log level for V logs (default \"0\") -V, --version Print version information.","title":"command line options"},{"location":"user-guide/#requirements","text":"Kube-router need to access kubernetes API server to get information on pods, services, endpoints, network policies etc. The very minimum information it requires is the details on where to access the kubernetes API server. This information can be passed as: kube-router --master=http://192.168.1.99:8080/` or `kube-router --kubeconfig=<path to kubeconfig file> If you run kube-router as agent on the node, ipset package must be installed on each of the nodes (when run as daemonset, container image is prepackaged with ipset) If you choose to use kube-router for pod-to-pod network connectivity then Kubernetes controller manager need to be configured to allocate pod CIDRs by passing --allocate-node-cidrs=true flag and providing a cluster-cidr (i.e. by passing --cluster-cidr=10.1.0.0/16 for e.g.) If you choose to run kube-router as daemonset in Kubernetes version below v1.15, both kube-apiserver and kubelet must be run with --allow-privileged=true option. In later Kubernetes versions, only kube-apiserver must be run with --allow-privileged=true option and if PodSecurityPolicy admission controller is enabled, you should create PodSecurityPolicy, allowing privileged kube-router pods. Additionally, when run in daemonset mode, it is highly recommended that you keep netfilter related userspace host tooling like iptables , ipset , and ipvsadm in sync with the versions that are distributed by Alpine inside the kube-router container. This will help avoid conflicts that can potentially arise when both the host's userspace and kube-router's userspace tooling modifies netfilter kernel definitions. See: this kube-router issue for more information. If you choose to use kube-router for pod-to-pod network connecitvity then Kubernetes cluster must be configured to use CNI network plugins. On each node CNI conf file is expected to be present as /etc/cni/net.d/10-kuberouter.conf bridge CNI plugin and host-local for IPAM should be used. A sample conf file that can be downloaded as wget -O /etc/cni/net.d/10-kuberouter.conf https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/cni/10-kuberouter.conf` Additionally, the aforementioned bridge and host-local CNI plugins need to exist for the container runtime to reference if you have kube-router manage the pod-to-pod network. Additionally, if you use hostPort 's on any of your pods, you'll need to install the hostport plugin. As of kube-router v2.1.X, these plugins will be installed to /opt/cni/bin for you during the initContainer phase if kube-router finds them missing. Most container runtimes will know to look for your plugins there by default, however, you may have to configure them if you are having problems with your pods coming up. containerd configuration cri-o configuration cri-dockerd configuration","title":"requirements"},{"location":"user-guide/#running-as-daemonset","text":"This is quickest way to deploy kube-router in Kubernetes ( dont forget to ensure the requirements above ). Just run: kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kube-router-all-service-daemonset.yaml Above will run kube-router as pod on each node automatically. You can change the arguments in the daemonset definition as required to suit your needs. Some sample deployment configuration can be found in our daemonset examples with different arguments used to select a set of the services kube-router should run.","title":"running as daemonset"},{"location":"user-guide/#running-as-agent","text":"You can choose to run kube-router as agent runnng on each node. For e.g if you just want kube-router to provide ingress firewall for the pods then you can start kube-router as: kube-router --master=http://192.168.1.99:8080/ --run-firewall=true --run-service-proxy=false --run-router=false","title":"running as agent"},{"location":"user-guide/#cleanup-configuration","text":"Please delete kube-router daemonset and then clean up all the configurations done (to ipvs, iptables, ipset, ip routes etc) by kube-router on the node by running below command.","title":"cleanup configuration"},{"location":"user-guide/#docker","text":"docker run --privileged --net=host \\ --mount type=bind,source=/lib/modules,target=/lib/modules,readonly \\ --mount type=bind,source=/run/xtables.lock,target=/run/xtables.lock,bind-propagation=rshared \\ cloudnativelabs/kube-router /usr/local/bin/kube-router --cleanup-config","title":"Docker"},{"location":"user-guide/#containerd","text":"$ ctr image pull docker.io/cloudnativelabs/kube-router:latest $ ctr run --privileged -t --net-host \\ --mount type=bind,src=/lib/modules,dst=/lib/modules,options=rbind:ro \\ --mount type=bind,src=/run/xtables.lock,dst=/run/xtables.lock,options=rbind:rw \\ docker.io/cloudnativelabs/kube-router:latest kube-router-cleanup /usr/local/bin/kube-router --cleanup-config","title":"containerd"},{"location":"user-guide/#trying-kube-router-as-alternative-to-kube-proxy","text":"If you have a kube-proxy in use, and want to try kube-router just for service proxy you can do kube-proxy --cleanup-iptables followed by kube-router --master=http://192.168.1.99:8080/ --run-service-proxy=true --run-firewall=false --run-router=false and if you want to move back to kube-proxy then clean up config done by kube-router by running kube-router --cleanup-config and run kube-proxy with the configuration you have.","title":"trying kube-router as alternative to kube-proxy"},{"location":"user-guide/#advertising-ips","text":"kube-router can advertise Cluster, External and LoadBalancer IPs to BGP peers. It does this by: locally adding the advertised IPs to the nodes' kube-dummy-if network interface advertising the IPs to its BGP peers To set the default for all services use the --advertise-cluster-ip , --advertise-external-ip and --advertise-loadbalancer-ip flags. To selectively enable or disable this feature per-service use the kube-router.io/service.advertise.clusterip , kube-router.io/service.advertise.externalip and kube-router.io/service.advertise.loadbalancerip annotations. e.g.: $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.clusterip=true\" $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.externalip=true\" $ kubectl annotate service my-advertised-service \"kube-router.io/service.advertise.loadbalancerip=true\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.clusterip=false\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.externalip=false\" $ kubectl annotate service my-non-advertised-service \"kube-router.io/service.advertise.loadbalancerip=false\" By combining the flags with the per-service annotations you can choose either a opt-in or opt-out strategy for advertising IPs. Advertising LoadBalancer IPs works by inspecting the services status.loadBalancer.ingress IPs that are set by external LoadBalancers like for example MetalLb. This has been successfully tested together with MetalLB in ARP mode.","title":"Advertising IPs"},{"location":"user-guide/#controlling-service-locality-traffic-policies","text":"Service availability both externally and locally (within the cluster) can be controlled via the Kubernetes standard Traffic Policies and via the custom kube-router service annotation: kube-router.io/service.local: true . Refer to the previously linked upstream Kubernetes documentation for more information on spec.internalTrafficPolicy and spec.externalTrafficPolicy . In order to keep backwards compatibility the kube-router.io/service.local: true annotation effectively overrides spec.internalTrafficPolicy and spec.externalTrafficPolicy and forces kube-router to behave as if both were set to Local .","title":"Controlling Service Locality / Traffic Policies"},{"location":"user-guide/#hairpin-mode","text":"Communication from a Pod that is behind a Service to its own ClusterIP:Port is not supported by default. However, it can be enabled per-service by adding the kube-router.io/service.hairpin= annotation, or for all Services in a cluster by passing the flag --hairpin-mode=true to kube-router. Additionally, the hairpin_mode sysctl option must be set to 1 for all veth interfaces on each node. This can be done by adding the \"hairpinMode\": true option to your CNI configuration and rebooting all cluster nodes if they are already running kubernetes. Hairpin traffic will be seen by the pod it originated from as coming from the Service ClusterIP if it is logging the source IP.","title":"Hairpin Mode"},{"location":"user-guide/#hairpin-mode-example","text":"10-kuberouter.conf { \"name\":\"mynet\", \"type\":\"bridge\", \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"hairpinMode\":true, \"ipam\": { \"type\":\"host-local\" } } To enable hairpin traffic for Service my-service : kubectl annotate service my-service \"kube-router.io/service.hairpin=\" If you want to also hairpin externalIPs declared for Service my-service (note, you must also either enable global hairpin or service hairpin (see above ^^^) for this to have an effect): kubectl annotate service my-service \"kube-router.io/service.hairpin.externalips=\"","title":"Hairpin Mode Example"},{"location":"user-guide/#snating-service-traffic","text":"By default, as traffic ingresses into the cluster, kube-router will source nat the traffic to ensure symmetric routing if it needs to proxy that traffic to ensure it gets to a node that has a service pod that is capable of servicing the traffic. This has a potential to cause issues when network policies are applied to that service since now the traffic will appear to be coming from a node in your cluster instead of the traffic originator. This is an issue that is common to all proxy's and all Kubernetes service proxies in general. You can read more information about this issue at: Source IP for Services In addition to the fix mentioned in the linked upstream documentation (using service.spec.externalTrafficPolicy ), kube-router also provides DSR , which by its nature preserves the source IP, to solve this problem. For more information see the section above.","title":"SNATing Service Traffic"},{"location":"user-guide/#load-balancing-scheduling-algorithms","text":"Kube-router uses LVS for service proxy. LVS support rich set of scheduling alogirthms . You can annotate the service to choose one of the scheduling alogirthms. When a service is not annotated round-robin scheduler is selected by default #For least connection scheduling use: $ kubectl annotate service my-service \"kube-router.io/service.scheduler=lc\" #For round-robin scheduling use: $ kubectl annotate service my-service \"kube-router.io/service.scheduler=rr\" #For source hashing scheduling use: $ kubectl annotate service my-service \"kube-router.io/service.scheduler=sh\" #For destination hashing scheduling use: $ kubectl annotate service my-service \"kube-router.io/service.scheduler=dh\"","title":"Load balancing Scheduling Algorithms"},{"location":"user-guide/#hostport-support","text":"If you would like to use HostPort functionality below changes are required in the manifest. By default kube-router assumes CNI conf file to be /etc/cni/net.d/10-kuberouter.conf . Add an environment variable KUBE_ROUTER_CNI_CONF_FILE to kube-router manifest and set it to /etc/cni/net.d/10-kuberouter.conflist Modify kube-router-cfg ConfigMap with CNI config that supports portmap as additional plug-in { \"cniVersion\":\"0.3.0\", \"name\":\"mynet\", \"plugins\":[ { \"name\":\"kubernetes\", \"type\":\"bridge\", \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"ipam\":{ \"type\":\"host-local\" } }, { \"type\":\"portmap\", \"capabilities\":{ \"snat\":true, \"portMappings\":true } } ] } Update init container command to create /etc/cni/net.d/10-kuberouter.conflist file Restart the container runtime For an e.g manifest please look at manifest with necessary changes required for HostPort functionality.","title":"HostPort support"},{"location":"user-guide/#ipvs-graceful-termination-support","text":"As of 0.2.6 we support experimental graceful termination of IPVS destinations. When possible the pods's TerminationGracePeriodSeconds is used, if it cannot be retrived for some reason the fallback period is 30 seconds and can be adjusted with --ipvs-graceful-period cli-opt graceful termination works in such a way that when kube-router receives a delete endpoint notification for a service it's weight is adjusted to 0 before getting deleted after he termination grace period has passed or the Active & Inactive connections goes down to 0.","title":"IPVS Graceful termination support"},{"location":"user-guide/#mtu","text":"The maximum transmission unit (MTU) determines the largest packet size that can be transmitted through your network. MTU for the pod interfaces should be set appropriately to prevent fragmentation and packet drops thereby achieving maximum performance. If auto-mtu is set to true ( auto-mtu is set to true by default as of kube-router 1.1), kube-router will determine right MTU for both kube-bridge and pod interfaces. If you set auto-mtu to false kube-router will not attempt to configure MTU. However you can choose the right MTU and set in the cni-conf.json section of the 10-kuberouter.conflist in the kube-router daemonsets . For e.g. cni-conf.json: | { \"cniVersion\":\"0.3.0\", \"name\":\"mynet\", \"plugins\":[ { \"name\":\"kubernetes\", \"type\":\"bridge\", \"mtu\": 1400, \"bridge\":\"kube-bridge\", \"isDefaultGateway\":true, \"ipam\":{ \"type\":\"host-local\" } } ] } If you set MTU yourself via the CNI config, you'll also need to set MTU of kube-bridge manually to the right value to avoid packet fragmentation in case of existing nodes on which kube-bridge is already created. On node reboot or in case of new nodes joining the cluster both the pod's interface and kube-bridge will be setup with specified MTU value.","title":"MTU"},{"location":"user-guide/#bgp-configuration","text":"Configuring BGP Peers","title":"BGP configuration"},{"location":"user-guide/#metrics","text":"Configure metrics gathering","title":"Metrics"}]}